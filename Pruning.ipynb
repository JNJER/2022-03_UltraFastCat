{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultra-fast categorization of image containing animals *in vivo* and *in computo*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! I am  [Jean-Nicolas Jérémie](https://github.com/JNJER) in this notebook, I will use the [Pytorch](https://pytorch.org/) library to run the networks and the [pandas](https://pandas.pydata.org/docs/getting_started/index.html) library to collect and display the results. This notebook focused on the training and the test of Deep convolutional neuronal networks (DCNNs) for ecological tasks such as detecting an animal in a natural scene.\n",
    "\n",
    "I uses transfer learning to train the DCNN, starting from a VGG16 network, taken from the `torchvision.models` library, pre-trained on the [Imagenet](http://image-net.org/) dataset which allows to perform label detection on naturals images for $K = 1000$ labels. Then re-train the whole network to perfom the same task but in a sub-set of $K = 1$ synset from the Imagenet dataset. The dataset I used here to train the networks is not multilabel (as we have the information about the occurence of only one synset on the scene), in order to train networks on independant task I choose to limit the output of the DCNN to $K = 1$ synset. \n",
    "\n",
    "In this notebook I test a pruning protocol where I progressively remove layers from a pre-trained VGG network. The network named vgg-1 has his last convolutional layer pruned, then I applied the same training process used for the VGG Gen network (see `UltraFastCat.ipynb`) with the dataset where the targets are animals. This protocol includes $12$ different deepness factors (from vgg-1 to vgg-12).\n",
    "\n",
    "The first part consist of the training of the networks and an analysis of various training parameters. Finally the last part of this notebook is dedicated to the test of the robustness of the resulted networks while appliying various geometric tranformations to the input or by varying the subjective complexity of the synset ('animal', 'birds', 'dog', 'cat...) \n",
    "\n",
    "This notebook was done during my thesis at the Neurosciences Institute of Timone (INT) under the supervision of [Laurent PERRINET](https://laurentperrinet.github.io/). It is curated in the following [github repo](https://github.com/JNJER/2022-03_UltraFastCat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography : Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:05.929665Z",
     "iopub.status.busy": "2022-05-16T06:51:05.928127Z",
     "iopub.status.idle": "2022-05-16T06:51:06.659609Z",
     "shell.execute_reply": "2022-05-16T06:51:06.658032Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%mkdir -p Prunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning process\n",
    "\n",
    "And finnaly, experiment_train.py, a pretty classic trainning script with pytorch. For further statistical analyses, we extract factors (like the accuracy and loss) in a `pandas` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:06.671336Z",
     "iopub.status.busy": "2022-05-16T06:51:06.669800Z",
     "iopub.status.idle": "2022-05-16T06:51:06.675181Z",
     "shell.execute_reply": "2022-05-16T06:51:06.673894Z"
    }
   },
   "outputs": [],
   "source": [
    "scriptname = 'Prunning/experiment_train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:06.690159Z",
     "iopub.status.busy": "2022-05-16T06:51:06.687676Z",
     "iopub.status.idle": "2022-05-16T06:51:06.755518Z",
     "shell.execute_reply": "2022-05-16T06:51:06.755904Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/experiment_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "from src.model import *\n",
    "from Prunning.vgg_maker import *\n",
    "def train_model(model, num_epochs, dataloaders, lr=args.lr, momentum=args.momentum, beta2=args.beta2, log_interval=100, **kwargs):\n",
    "    \n",
    "    model.to(device)\n",
    "    if beta2 > 0.: \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(momentum, beta2)) #, amsgrad=amsgrad)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum) # to set training variables\n",
    "\n",
    "    df_train = pd.DataFrame([], columns=['epoch', 'avg_loss', 'avg_acc', 'avg_loss_val', 'avg_acc_val', 'device_type']) \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_train = 0\n",
    "        acc_train = 0\n",
    "        for i, (images, labels) in enumerate(dataloaders['train']):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs[:,0], labels.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item() * images.size(0)\n",
    "            preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "            acc_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "        avg_loss = loss_train / dataset_sizes[task]['train']\n",
    "        avg_acc = acc_train / dataset_sizes[task]['train']\n",
    "           \n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            acc_val = 0\n",
    "            for i, (images, labels) in enumerate(dataloaders['val']):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs[:,0], labels.float())\n",
    "\n",
    "                loss_val += loss.item() * images.size(0)\n",
    "                preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "                acc_val += torch.sum(preds == labels.data)\n",
    "        \n",
    "            avg_loss_val = loss_val / dataset_sizes[task]['val']\n",
    "            avg_acc_val = acc_val / dataset_sizes[task]['val']\n",
    "        \n",
    "        df_train.loc[epoch] = {'epoch':epoch, 'avg_loss':avg_loss, 'avg_acc':float(avg_acc),\n",
    "                               'avg_loss_val':avg_loss_val, 'avg_acc_val':float(avg_acc_val), 'device_type':device.type}\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} : train= loss: {avg_loss:.4f} / acc : {avg_acc:.4f} - val= loss : {avg_loss_val:.4f} / acc : {avg_acc_val:.4f}\")\n",
    "\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, df_train\n",
    "\n",
    "# https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451\n",
    "criterion = nn.BCEWithLogitsLoss() #binary_cross_entropy_with_logits\n",
    "\n",
    "# Training and saving the network\n",
    "\n",
    "n_output = 1\n",
    "len_pruning = 10\n",
    "\n",
    "# Downloading the model\n",
    "model_filenames = {}\n",
    "models_vgg = {}\n",
    "\n",
    "models_vgg['VGG_Gen'] = torchvision.models.vgg16(pretrained=True)  \n",
    "num_features = models_vgg['VGG_Gen'].classifier[-1].in_features\n",
    "features = list(models_vgg['VGG_Gen'].classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, n_output)]) # Add our layer with 10 outputs\n",
    "models_vgg['VGG_Gen'].classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "  \n",
    "\n",
    "task = 'animal'\n",
    "model_filenames['VGG_Gen'] =  args.model_path + \"vgg16_gen_\"+ task + \".pt\"\n",
    "\n",
    "m = 0\n",
    "input_lin_1 = 12544\n",
    "vgg_head = torchvision.models.vgg16(pretrained=True)\n",
    "all_models = ['vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8','vgg-9','vgg-10', 'vgg-11', 'vgg-12']\n",
    "goal =  list(vgg_head.features.children())\n",
    "for model_name in all_models:\n",
    "    model_filenames[model_name] = str(args.model_path +model_name+'.pt')\n",
    "    models_vgg[model_name] = torchvision.models.vgg16(pretrained=True)\n",
    "    if len(goal) > 28 :\n",
    "        del goal[24:26] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal) #get the new features parameters in the model\n",
    "    elif len(goal) > 26:\n",
    "        del goal[24:26] # remove the three last Conv2d and ReLU layers\n",
    "        del goal[-1] # remove the last MaxPool2d layer when no Conv2d or ReLU layer left\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal) \n",
    "    elif len(goal) > 20: # same as above for the next three Conv2d and ReLU layers\n",
    "        del goal[19:21] \n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal) \n",
    "    elif len(goal) > 17:\n",
    "        del goal[17:19] # remove the three last Conv2d and ReLU layers\n",
    "        del goal[-1] # remove the last MaxPool2d layer when no Conv2d or ReLU layer left\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    elif len(goal) > 13:\n",
    "        del goal[11:13] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    elif len(goal) > 10:\n",
    "        del goal[10:12] # remove the three last Conv2d and ReLU layers\n",
    "        del goal[-1] # remove the last MaxPool2d layer when no Conv2d or ReLU layer left\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    elif len(goal)>9:\n",
    "        del goal[6:8] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    elif len(goal)>7:\n",
    "        del goal[3:6] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    else:\n",
    "        del goal[2:4] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "        \n",
    "        \n",
    "    if model_name in ['vgg-6','vgg-7','vgg-8']:   \n",
    "        num_features = models_vgg[model_name].classifier[6].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children())[:-1] # Remove last layer\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1, num_features)) # Add your custom input layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with n outputs\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "        \n",
    "    elif model_name in ['vgg-9','vgg-10']:\n",
    "        num_features = models_vgg[model_name].classifier[6].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children())[:-1] # Remove last layer\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1//2, num_features)) # Add your custom input layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with n outputs\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "        \n",
    "    elif model_name in ['vgg-11','vgg-12']:\n",
    "        num_features = models_vgg[model_name].classifier[6].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children())[:-1] # Remove last layer\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1//4, num_features)) # Add your custom input layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with n outputs\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "\n",
    "    else : \n",
    "        num_features = models_vgg[model_name].classifier[6].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children())[:-1] # Remove last layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with n outputs\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier     \n",
    "        m += 1\n",
    "#models_vgg['VGG_Test'] = torchvision.models.vgg16(pretrained=False)     # Test why VGG_maker fail... conv part +++\n",
    "#models_vgg['VGG_Test'].features = models_vgg['vgg-6'].features \n",
    "#models_vgg['VGG_Test'].classifier = vgg16_classifier(pretrained= False, deepness = \"5\").classifier\n",
    "#model_filenames['VGG_Test'] =  args.model_path + \"vgg_test.pt\"  \n",
    "\n",
    "    model_filenames[model_name] = str(args.model_path +model_name+\"_\"+ task +'.pt') # Uncomment to load dog models\n",
    "\n",
    "for model_name in models_vgg:\n",
    "    if os.path.isfile(model_filenames[model_name]):\n",
    "        print(\"Loading pretrained model for..\", model_name, ' from', model_filenames[model_name])\n",
    "        #print(\"Resume_training : \", resume_training)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            models_vgg[model_name].load_state_dict(torch.load(model_filenames[model_name])) #on GPU\n",
    "        else:\n",
    "            models_vgg[model_name].load_state_dict(torch.load(model_filenames[model_name], map_location=torch.device('cpu'))) #on CPU\n",
    "    else :\n",
    "        p = 0\n",
    "        filename = f'results/{datetag}_{HOST}_train_{model_name}.json'\n",
    "        print(\"Re-training pretrained model...\", model_filenames[model_name])\n",
    "        since = time.time()\n",
    "        print(f\"Traning {model_name}, image_size = {args.image_size}, p (Grayscale) = {p}\")\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, p=p)\n",
    "        models_vgg[model_name], df_train = train_model(models_vgg[model_name], num_epochs=args.num_epochs,\n",
    "                                                    dataloaders=dataloaders[task])\n",
    "        torch.save(models_vgg[model_name].state_dict(), model_filenames[model_name])\n",
    "        df_train.to_json(filename)\n",
    "        elapsed_time = time.time() - since\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:06.764417Z",
     "iopub.status.busy": "2022-05-16T06:51:06.763918Z",
     "iopub.status.idle": "2022-05-16T06:51:11.794729Z",
     "shell.execute_reply": "2022-05-16T06:51:11.795931Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average accuracy and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display both average accuracy and loss during the training phase and during the validation one : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:11.845534Z",
     "iopub.status.busy": "2022-05-16T06:51:11.843936Z",
     "iopub.status.idle": "2022-05-16T06:51:11.849226Z",
     "shell.execute_reply": "2022-05-16T06:51:11.850416Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m datetag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-01-07\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_models\u001b[49m:\n\u001b[1;32m      3\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filename):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_models' is not defined"
     ]
    }
   ],
   "source": [
    "datetag = '2022-01-07'\n",
    "for model_name in all_models:\n",
    "    filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
    "    if os.path.isfile(filename):\n",
    "        df_train = pd.read_json(filename)\n",
    "        fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi/2))\n",
    "        ax = df_train['avg_loss'].plot(lw=2, marker='.', markersize=10)\n",
    "        ax = df_train['avg_loss_val'].plot(lw=2, marker='.', markersize=10)\n",
    "        ax.legend([\"avg_loss\", \"avg_loss_val\"], fontsize=18);\n",
    "        ax.set_xlabel(\"Epoch\", size=18)\n",
    "        ax.spines['left'].set_position(('axes', -0.01))\n",
    "        ax.set_xlim(-0.5, args.num_epochs)\n",
    "        ax.grid(which='both')\n",
    "        for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "        ax.set_ylim(0., 1.1)\n",
    "        axs.set_title(f'Average values of the loss by epoch : {filename}' , size = 20)\n",
    "        ax.get_legend().remove()\n",
    "        fig.legend(bbox_to_anchor=(1.05, .5), loc='lower right', fontsize = 20)\n",
    "    else:\n",
    "        print('no such files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:11.881504Z",
     "iopub.status.busy": "2022-05-16T06:51:11.879897Z",
     "iopub.status.idle": "2022-05-16T06:51:11.885154Z",
     "shell.execute_reply": "2022-05-16T06:51:11.886386Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_models\u001b[49m:\n\u001b[1;32m      2\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filename):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_models' is not defined"
     ]
    }
   ],
   "source": [
    "for model_name in all_models:\n",
    "    filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
    "    if os.path.isfile(filename):\n",
    "        df_train = pd.read_json(filename)\n",
    "        fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi/2))\n",
    "        ax = df_train['avg_acc'].plot(lw=2, marker='.', markersize=10)\n",
    "        ax = df_train['avg_acc_val'].plot(lw=2, marker='.', markersize=10)\n",
    "        ax.legend([\"avg_acc\", \"avg_acc_val\"], fontsize=18);\n",
    "        ax.set_xlabel(\"Epoch\", size=18)\n",
    "        ax.spines['left'].set_position(('axes', -0.01))\n",
    "        #ax.set_ylim(0.70, .992)\n",
    "        ax.set_yscale(\"logit\", one_half=\"1/2\", use_overline=True)\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xlim(-0.5, args.num_epochs+.5)\n",
    "        for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "        axs.set_title(f'Average values of the accuracy by epoch : {filename}' , size = 20)\n",
    "        ax.get_legend().remove()\n",
    "        fig.legend(bbox_to_anchor=(1.05, .5), loc='lower right', fontsize=20)\n",
    "    else:\n",
    "        print('no such file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Bonus: Scan of some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:11.892854Z",
     "iopub.status.busy": "2022-05-16T06:51:11.891284Z",
     "iopub.status.idle": "2022-05-16T06:51:11.894316Z",
     "shell.execute_reply": "2022-05-16T06:51:11.895241Z"
    }
   },
   "outputs": [],
   "source": [
    "scriptname = 'experiment_scan.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:11.901466Z",
     "iopub.status.busy": "2022-05-16T06:51:11.900180Z",
     "iopub.status.idle": "2022-05-16T06:51:11.923169Z",
     "shell.execute_reply": "2022-05-16T06:51:11.924350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiment_scan.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_transfer_learning.model import *\n",
    "\n",
    "scan_dicts= {'batch_size' : [8, 13, 21, 34, 55],\n",
    "             'lr': args.lr * np.logspace(-1, 1, 7, base=10),\n",
    "             'momentum': 1 - np.logspace(-3, -.5, 7, base=10),\n",
    "             'beta2': 1 - np.logspace(-5, -1, 7, base=10),\n",
    "            }\n",
    "\n",
    "def main(N_avg=10, num_epochs=args.num_epochs//4):\n",
    "    from experiment_train import train_model\n",
    "\n",
    "    for key in scan_dicts:\n",
    "        filename = f'results/{datetag}_train_scan_{key}_{args.HOST}.json'\n",
    "        print(f'{filename=}')\n",
    "        if os.path.isfile(filename):\n",
    "            df_scan = pd.read_json(filename)\n",
    "        else:\n",
    "            i_trial = 0\n",
    "            measure_columns = [key, 'avg_loss_val', 'avg_acc_val', 'time']\n",
    "\n",
    "            df_scan = pd.DataFrame([], columns=measure_columns) \n",
    "            for i_trial, value in enumerate(scan_dicts[key]):\n",
    "                new_kwarg = {key: value}\n",
    "                print('trial', i_trial, ' /', len(scan_dicts[key]))\n",
    "                print('new_kwarg', new_kwarg)\n",
    "                # Training and saving the network\n",
    "                models_vgg_ = torchvision.models.vgg16(pretrained=True)\n",
    "                # Freeze training for all layers\n",
    "                # Newly created modules have require_grad=True by default\n",
    "                for param in models_vgg_.features.parameters():\n",
    "                    param.require_grad = False \n",
    "\n",
    "                num_features = models_vgg_.classifier[-1].in_features\n",
    "                features = list(models_vgg_.classifier.children())[:-1] # Remove last layer\n",
    "                features.extend([nn.Linear(num_features, n_output)]) # Add our layer with `n_output` outputs\n",
    "                models_vgg_.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "                since = time.time()\n",
    "\n",
    "                (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, p=0, **new_kwarg)\n",
    "                models_vgg_, df_train = train_model(models_vgg_, num_epochs=num_epochs, dataloaders=dataloaders, **new_kwarg)\n",
    "\n",
    "                elapsed_time = time.time() - since\n",
    "                print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "\n",
    "                df_scan.loc[i_trial] = {key:value, 'avg_loss_val':df_train.iloc[-N_avg:-1]['avg_loss_val'].mean(), \n",
    "                                   'avg_acc_val':df_train.iloc[-N_avg:-1]['avg_acc_val'].mean(), 'time':elapsed_time}\n",
    "                print(df_scan.loc[i_trial])\n",
    "                i_trial += 1\n",
    "            df_scan.to_json(filename)\n",
    "\n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:11.933996Z",
     "iopub.status.busy": "2022-05-16T06:51:11.932477Z",
     "iopub.status.idle": "2022-05-16T06:51:12.015586Z",
     "shell.execute_reply": "2022-05-16T06:51:12.016776Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DCNN_transfer_learning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/experiment_scan.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDCNN_transfer_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m scan_dicts\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m : [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m55\u001b[39m],\n\u001b[1;32m      6\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: args\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m, base\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m      7\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m7\u001b[39m, base\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m      8\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m, base\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m      9\u001b[0m             }\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(N_avg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DCNN_transfer_learning'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPython CPU timings (estimated):\n",
      "  User   :       0.05 s.\n",
      "  System :       0.00 s.\n",
      "Wall time:       0.08 s.\n"
     ]
    }
   ],
   "source": [
    "%run -int {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:12.034673Z",
     "iopub.status.busy": "2022-05-16T06:51:12.033144Z",
     "iopub.status.idle": "2022-05-16T06:51:12.038270Z",
     "shell.execute_reply": "2022-05-16T06:51:12.039507Z"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scan_dicts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mscan_dicts\u001b[49m:\n\u001b[1;32m      2\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_scan_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(filename)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scan_dicts' is not defined"
     ]
    }
   ],
   "source": [
    "for key in scan_dicts:\n",
    "    filename = f'results/{datetag}_train_scan_{key}_{args.HOST}.json'\n",
    "    print(filename)\n",
    "    df_scan = pd.read_json(filename)\n",
    "    print(df_scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we display both average accuracy and loss during the training phase and during the validation one : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the new networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the networks seems ready for the comparison. The second part of this notebook offers a comparison between the three re-trained version of the same network VGG16 based on a Wordnet semantic from the Imagenet database wich allows to work on naturals images for $1$ synsets. \n",
    "\n",
    "Since the networks need to be re-train in order to perform a categorization with fewer convolutional layers I did not compute the accuracy for the pre-trained VGG network trained on the [Imagenet](http://image-net.org/) dataset wich allows to work on naturals images for $1000$ labels, taken from the `torchvision.models` library in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp1\"></a>\n",
    "## Experiment 1: Image processing and recognition for differents labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:12.046334Z",
     "iopub.status.busy": "2022-05-16T06:51:12.045014Z",
     "iopub.status.idle": "2022-05-16T06:51:12.047631Z",
     "shell.execute_reply": "2022-05-16T06:51:12.048563Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scriptname =  'Prunning/experiment_basic.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:12.055228Z",
     "iopub.status.busy": "2022-05-16T06:51:12.053967Z",
     "iopub.status.idle": "2022-05-16T06:51:12.077678Z",
     "shell.execute_reply": "2022-05-16T06:51:12.076389Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/experiment_basic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "datetag = '2022-02-08'\n",
    "filename = f'results/{datetag}_results_1_del_lay_{HOST}.json'\n",
    "print(f'{filename=}')\n",
    "task = 'animal'\n",
    "\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_del_lay = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_del_lay = pd.DataFrame([], columns=['model', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "            # image preprocessing\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg:\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0)\n",
    "                    percentage = float((torch.sigmoid(out) * 100).detach().cpu().numpy()[0])\n",
    "                    top_1 = 'target' if percentage>50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df_del_lay.loc[i_trial] = {'model':model_name, 'top_1':top_1, 'goal':goal, 'likelihood':percentage, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                      'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type}\n",
    "                print(f'The {model_name} model categorize an animal with {percentage:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "    df_del_lay.to_json(filename)\n",
    "main()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:12.087491Z",
     "iopub.status.busy": "2022-05-16T06:51:12.085977Z",
     "iopub.status.idle": "2022-05-16T06:51:15.025982Z",
     "shell.execute_reply": "2022-05-16T06:51:15.027157Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_basic.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      6\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:15.045377Z",
     "iopub.status.busy": "2022-05-16T06:51:15.043857Z",
     "iopub.status.idle": "2022-05-16T06:51:15.048929Z",
     "shell.execute_reply": "2022-05-16T06:51:15.050138Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_1_del_lay_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m df_del_lay \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      3\u001b[0m df_del_lay\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOST' is not defined"
     ]
    }
   ],
   "source": [
    "filename = f'results/{datetag}_results_1_del_lay_{HOST}.json'\n",
    "df_del_lay = pd.read_json(filename)\n",
    "df_del_lay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I displayed the images in various conditions with the likelihood at the output of the network as the `y_label`, the label found by the network and the name of the networks as the `x_label`. If the labels are in green the categorization made by the network is correct (detect a target:'animal' in the scene when there is indeed an animal in the scene), if the labels are red the network failed the categorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the higher likelihood of the networks : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:15.060302Z",
     "iopub.status.busy": "2022-05-16T06:51:15.058727Z",
     "iopub.status.idle": "2022-05-16T06:51:15.086406Z",
     "shell.execute_reply": "2022-05-16T06:51:15.087653Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_del_lay\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_del_lay.sort_values(by=['likelihood'], ascending=False).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_del_lay.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_del_lay.loc[idx]['top_1'] == df_del_lay.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_del_lay.loc[idx]['top_1'] + ' | ' + df_del_lay.loc[idx]['model'], color=color)\n",
    "    likelihood = df_del_lay.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the lowest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:15.115521Z",
     "iopub.status.busy": "2022-05-16T06:51:15.113973Z",
     "iopub.status.idle": "2022-05-16T06:51:15.120379Z",
     "shell.execute_reply": "2022-05-16T06:51:15.119071Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_del_lay\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_del_lay.sort_values(by=['likelihood'], ascending=True).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_del_lay.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_del_lay.loc[idx]['top_1'] == df_del_lay.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_del_lay.loc[idx]['top_1'] + ' | ' + df_del_lay.loc[idx]['model'], color=color)\n",
    "    likelihood = df_del_lay.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)  \n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:15.151788Z",
     "iopub.status.busy": "2022-05-16T06:51:15.150245Z",
     "iopub.status.idle": "2022-05-16T06:51:15.156634Z",
     "shell.execute_reply": "2022-05-16T06:51:15.155334Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m fig_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m cmap \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mget_cmap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m color_dict \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({k:cmap(i\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(df_del_lay[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())) \u001b[38;5;28;01mfor\u001b[39;00m i,k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_del_lay[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())})\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m models_vgg:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "fig_width = 15\n",
    "results = []\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "color_dict = pd.Series({k:cmap(i/len(df_del_lay['model'].unique())) for i,k in enumerate(df_del_lay['model'].unique())})\n",
    "for name in models_vgg:\n",
    "    results.append(f1_score(df_del_lay[df_del_lay['model']==name][\"top_1\"],\n",
    "                            df_del_lay[df_del_lay['model']==name][\"goal\"],average='micro'))\n",
    "df_acc = pd.DataFrame({'': results},index = models_vgg)\n",
    "ax = df_acc.plot.bar(rot=30, figsize=(fig_width//1.2, fig_width//2.5), fontsize = 18, color=color_dict['vgg-6'])\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='k')\n",
    "ax.get_legend().remove()\n",
    "plt.legend(bbox_to_anchor=(1.15, .5), loc='right')\n",
    "ax.set_ylabel('Accuracy', size= 24, fontweight='bold' )\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, fontsize=15.5, fmt='%.3f', rotation=90, color='black', fontweight='bold')\n",
    "plt.xticks(fontsize=20,  fontweight='bold')\n",
    "plt.yticks(fontsize=20,  fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('IJCNN_fig.7.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:15.183094Z",
     "iopub.status.busy": "2022-05-16T06:51:15.181569Z",
     "iopub.status.idle": "2022-05-16T06:51:15.186685Z",
     "shell.execute_reply": "2022-05-16T06:51:15.187937Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;28mlen\u001b[39m(models_vgg\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m*\u001b[39mphi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sharey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax, model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axs, models_vgg\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m      3\u001b[0m     df_del_lay[df_del_lay[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mhist(bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m350\u001b[39m, lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, label\u001b[38;5;241m=\u001b[39mmodel_name,ax\u001b[38;5;241m=\u001b[39max, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi/2), sharex=True, sharey=True)\n",
    "for ax, model_name in zip(axs, models_vgg.keys()):\n",
    "    df_del_lay[df_del_lay['model']==model_name]['time'].plot.hist(bins=350, lw=1, label=model_name,ax=ax, density=True)\n",
    "    ax.set_xlim(df_del_lay['time'].quantile(.01), df_del_lay['time'].quantile(.99))\n",
    "    ax.legend(bbox_to_anchor=(1.21, .35), loc='lower right', fontsize=15)\n",
    "    ax.grid(which='both', axis='y')\n",
    "    ax.set_xlabel('Processing time (s): ' + model_name, size=22)\n",
    "axs[0].set_title('Distribution of the Processing time (s). Processed on : ' + args.HOST + '_' + str(df_del_lay['device_type'][0]), size = 20);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We tested the pruned networks re-trained to categorize an animal in a scene on our Imagenet dataset. VGG Gen gets the best acuracy on this task, with a slight drop at vgg-1, followed by a flat phase between vgg-1 and vgg-8. Then another drop between vgg-9 and vgg-12 where the networks stands close to the chance level. The accuracies of the networks stay similar to the performances found by\n",
    "[Serre & al](http://vision.stanford.edu/teaching/cs131_fall1415/lectures/Serre2007.pdf), this is not a surprise as their model relies on low level features. Note that the computing time necessary to perform the categorization decreases with the accuracy of the network ({\\sc VGG Gen} = 0.005 +/- 0.0001, {\\sc vgg-8} = 0.003 +/- 0.0001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp2\"></a>\n",
    "## Experiment 2: Image processing and recognition for differents resolutions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:15.193589Z",
     "iopub.status.busy": "2022-05-16T06:51:15.192563Z",
     "iopub.status.idle": "2022-05-16T06:51:15.194862Z",
     "shell.execute_reply": "2022-05-16T06:51:15.195631Z"
    }
   },
   "outputs": [],
   "source": [
    "scriptname = 'Prunning/experiment_downsample.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:15.201735Z",
     "iopub.status.busy": "2022-05-16T06:51:15.200600Z",
     "iopub.status.idle": "2022-05-16T06:51:15.224313Z",
     "shell.execute_reply": "2022-05-16T06:51:15.225408Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/experiment_downsample.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "filename = f'results/{datetag}_results_2_del_lay_{HOST}.json'\n",
    "task = 'animal'\n",
    " \n",
    "\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_downsample_del = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_downsample_del = pd.DataFrame([], columns=['model','task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'image_size', 'top_1', 'device']) \n",
    "        # image preprocessing\n",
    "        for image_size in args.image_sizes:\n",
    "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=image_size)\n",
    "            print(f'Résolution de {image_size=}')\n",
    "\n",
    "            # Displays the input image of the model \n",
    "            for i_image, (data, label) in enumerate(image_datasets[task]['test']):\n",
    "                for model_name in models_vgg:\n",
    "                    model = models_vgg[model_name].to(device)\n",
    "                    model.eval()\n",
    "                    tic = time.time()\n",
    "                    out = model(data.unsqueeze(0).to(device)).squeeze(0)\n",
    "                    percentage = (torch.sigmoid(out) * 100).detach().cpu().numpy()[0]\n",
    "                    elapsed_time = time.time() - tic\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    top_1 = 'target' if percentage>50 else 'distractor'\n",
    "                    df_downsample_del.loc[i_trial] = {'model':model_name, 'task':image_datasets[task]['test'].classes[label], 'top_1':top_1, 'goal':goal, 'likelihood':percentage, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                   'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'image_size': image_size, 'device':str(device)}\n",
    "                    print(f'The {model_name} model categorize {task} with {percentage:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "\n",
    "                    i_trial += 1\n",
    "    df_downsample_del.to_json(filename)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:15.235491Z",
     "iopub.status.busy": "2022-05-16T06:51:15.233960Z",
     "iopub.status.idle": "2022-05-16T06:51:18.137633Z",
     "shell.execute_reply": "2022-05-16T06:51:18.136652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_downsample.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      5\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:18.152309Z",
     "iopub.status.busy": "2022-05-16T06:51:18.150777Z",
     "iopub.status.idle": "2022-05-16T06:51:18.157088Z",
     "shell.execute_reply": "2022-05-16T06:51:18.155740Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_2_del_lay_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m df_downsample_del \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      3\u001b[0m df_downsample_del\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOST' is not defined"
     ]
    }
   ],
   "source": [
    "filename = f'results/{datetag}_results_2_del_lay_{HOST}.json'\n",
    "df_downsample_del = pd.read_json(filename)\n",
    "df_downsample_del"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image display "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the highest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:18.185326Z",
     "iopub.status.busy": "2022-05-16T06:51:18.183766Z",
     "iopub.status.idle": "2022-05-16T06:51:18.188913Z",
     "shell.execute_reply": "2022-05-16T06:51:18.190147Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_downsample_del\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_downsample_del.sort_values(by=['likelihood'], ascending=False).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_downsample_del.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_downsample_del.loc[idx]['top_1'] == df_downsample_del.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_downsample_del.loc[idx]['top_1'] + ' | ' + df_downsample_del.loc[idx]['model'] + ' | ' + str(df_downsample_del.loc[idx]['image_size']), color=color)\n",
    "    likelihood = df_downsample_del.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the lowest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:18.217884Z",
     "iopub.status.busy": "2022-05-16T06:51:18.216345Z",
     "iopub.status.idle": "2022-05-16T06:51:18.221472Z",
     "shell.execute_reply": "2022-05-16T06:51:18.222652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_downsample_del\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_downsample_del.sort_values(by=['likelihood'], ascending=True).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_downsample_del.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_downsample_del.loc[idx]['top_1'] == df_downsample_del.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_downsample_del.loc[idx]['top_1'] + ' | ' + df_downsample_del.loc[idx]['model'] + ' | ' + str(df_downsample_del.loc[idx]['image_size']), color=color)\n",
    "    likelihood = df_downsample_del.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)  \n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:18.252078Z",
     "iopub.status.busy": "2022-05-16T06:51:18.250663Z",
     "iopub.status.idle": "2022-05-16T06:51:18.255304Z",
     "shell.execute_reply": "2022-05-16T06:51:18.256583Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#names_diff = ['vgg16_gen','vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8'] \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, f1_score\n\u001b[0;32m----> 4\u001b[0m df_acc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({model_name: {image_size: accuracy_score(df_downsample_del[(df_downsample_del[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name) \u001b[38;5;241m&\u001b[39m (df_downsample_del[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mimage_size)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_1\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      5\u001b[0m                                                                df_downsample_del[(df_downsample_del[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name) \u001b[38;5;241m&\u001b[39m (df_downsample_del[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mimage_size)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m image_size \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mimage_sizes} \n\u001b[1;32m      7\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models_vgg})\n\u001b[1;32m      9\u001b[0m ax \u001b[38;5;241m=\u001b[39m df_acc\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     10\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "fig_width = 35\n",
    "#names_diff = ['vgg16_gen','vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8'] \n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "df_acc = pd.DataFrame({model_name: {image_size: accuracy_score(df_downsample_del[(df_downsample_del['model']==model_name) & (df_downsample_del['image_size']==image_size)][\"top_1\"], \n",
    "                                                               df_downsample_del[(df_downsample_del['model']==model_name) & (df_downsample_del['image_size']==image_size)][\"goal\"])\n",
    "                                    for image_size in args.image_sizes} \n",
    "                       for model_name in models_vgg})\n",
    "\n",
    "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=30)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=18, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.05, .5), loc='center', fontsize=18)\n",
    "#ax.set_title(f'Experiment 2 - image sizes = {args.image_sizes}', size=20)\n",
    "ax.set_ylabel('F1 Score', size=32)\n",
    "ax.set_xlabel('Image sizes', size=32)\n",
    "#plt.show();\n",
    "plt.savefig('del_lay_diff_size.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:18.281036Z",
     "iopub.status.busy": "2022-05-16T06:51:18.279462Z",
     "iopub.status.idle": "2022-05-16T06:51:18.284624Z",
     "shell.execute_reply": "2022-05-16T06:51:18.285890Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[43mdf_acc\u001b[49m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[1;32m      2\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mhlines(xmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.5\u001b[39m, xmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(models_vgg\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.5\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, ls\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, ec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchance level\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_acc' is not defined"
     ]
    }
   ],
   "source": [
    "ax = df_acc.T.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.grid(which='both', axis='y')\n",
    "for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "ax.set_title(f'Experiment 2 - image sizes = {args.image_sizes}', size=20)\n",
    "#ax.set_xticks([])\n",
    "ax.set_ylabel('F1 score', size=32)\n",
    "ax.set_xlabel('Models', size=32)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:18.313632Z",
     "iopub.status.busy": "2022-05-16T06:51:18.312092Z",
     "iopub.status.idle": "2022-05-16T06:51:18.317223Z",
     "shell.execute_reply": "2022-05-16T06:51:18.318451Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39mphi))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m color, model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(colors, models_vgg\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m      3\u001b[0m     axs \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mviolinplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mdf_downsample_del, inner\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquartile\u001b[39m\u001b[38;5;124m\"\u001b[39m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi))\n",
    "for color, model_name in zip(colors, models_vgg.keys()):\n",
    "    axs = sns.violinplot(x=\"image_size\", y=\"time\", data=df_downsample_del, inner=\"quartile\", hue='model')\n",
    "    axs.set_title('Processed on : ' + args.HOST + '_' + str(df_downsample_del['device'][0]), size = 20)\n",
    "    axs.set_ylabel('Computation time (s)', size=18)\n",
    "    axs.set_xlabel('Trial', size=18)\n",
    "    axs.set_yscale('log')\n",
    "    axs.grid(which='both', axis='y')\n",
    "    for side in ['top', 'right'] :axs.spines[side].set_visible(False)\n",
    "h, l = axs.get_legend_handles_labels()\n",
    "axs.legend(h[:5], l[:5], loc='upper center', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The categorization performance keeps relatively the same organization with the transformation as the VGG Gen stay the best networks and the VGG-12 the worst on this task. Surprisingly, the robustness of the networks does not seem to be directly related to their depth as one might expect. Shorter networks rely on lower levels of features for their categorization, so this may explains why networks like Vgg-3 are more robust to this transformation than VGG Gen even if their overall performances is not as good as VGG Gen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Image processing and recognition on grayscale images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:18.325034Z",
     "iopub.status.busy": "2022-05-16T06:51:18.323473Z",
     "iopub.status.idle": "2022-05-16T06:51:18.326449Z",
     "shell.execute_reply": "2022-05-16T06:51:18.327365Z"
    }
   },
   "outputs": [],
   "source": [
    "scriptname = 'Prunning/experiment_grayscale.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:18.333525Z",
     "iopub.status.busy": "2022-05-16T06:51:18.332263Z",
     "iopub.status.idle": "2022-05-16T06:51:18.357350Z",
     "shell.execute_reply": "2022-05-16T06:51:18.358471Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/experiment_grayscale.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "filename = f'results/{datetag}_results_3_del_lay_{args.HOST}.json'\n",
    "print(f'{filename=}')\n",
    "task = 'animal'\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_gray_del_lay = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_gray_del_lay = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "            # image preprocessing\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1, p=1)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg:\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0)\n",
    "                    percentage = float((torch.sigmoid(out) * 100).detach().cpu().numpy()[0])\n",
    "                    top_1 = 'target' if percentage>50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df_gray_del_lay.loc[i_trial] = {'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal, 'likelihood':percentage, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                      'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type}\n",
    "                print(f'The {model_name} model categorize {model_task} with {percentage:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "    df_gray_del_lay.to_json(filename)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:18.368657Z",
     "iopub.status.busy": "2022-05-16T06:51:18.367098Z",
     "iopub.status.idle": "2022-05-16T06:51:21.664921Z",
     "shell.execute_reply": "2022-05-16T06:51:21.666093Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_grayscale.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      5\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:21.681806Z",
     "iopub.status.busy": "2022-05-16T06:51:21.680282Z",
     "iopub.status.idle": "2022-05-16T06:51:21.685387Z",
     "shell.execute_reply": "2022-05-16T06:51:21.686614Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_3_del_lay_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m df_gray_del_lay \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      3\u001b[0m df_gray_del_lay\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOST' is not defined"
     ]
    }
   ],
   "source": [
    "filename = f'results/{datetag}_results_3_del_lay_{HOST}.json'\n",
    "df_gray_del_lay = pd.read_json(filename)\n",
    "df_gray_del_lay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the highest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:21.716027Z",
     "iopub.status.busy": "2022-05-16T06:51:21.714441Z",
     "iopub.status.idle": "2022-05-16T06:51:21.719609Z",
     "shell.execute_reply": "2022-05-16T06:51:21.720864Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_gray_del_lay\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_gray_del_lay.sort_values(by=['likelihood'], ascending=False).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_gray_del_lay.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address, pilmode=\"L\"), cmap='gray')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_gray_del_lay.loc[idx]['top_1'] == df_gray_del_lay.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_gray_del_lay.loc[idx]['top_1'] + ' | ' + df_gray_del_lay.loc[idx]['model'], color=color)\n",
    "    likelihood = df_gray_del_lay.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the lowest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:21.749590Z",
     "iopub.status.busy": "2022-05-16T06:51:21.748043Z",
     "iopub.status.idle": "2022-05-16T06:51:21.753191Z",
     "shell.execute_reply": "2022-05-16T06:51:21.754439Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_gray_del_lay\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_gray_del_lay.sort_values(by=['likelihood'], ascending=True).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_gray_del_lay.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address, pilmode=\"L\"), cmap='gray')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_gray_del_lay.loc[idx]['top_1'] == df_gray_del_lay.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_gray_del_lay.loc[idx]['top_1'] + ' | ' + df_gray_del_lay.loc[idx]['model'], color=color)\n",
    "    likelihood = df_gray_del_lay.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(df_gray_del_lay.loc[idx]['goal'], color=color)  \n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:21.783366Z",
     "iopub.status.busy": "2022-05-16T06:51:21.781802Z",
     "iopub.status.idle": "2022-05-16T06:51:21.787012Z",
     "shell.execute_reply": "2022-05-16T06:51:21.788212Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models_vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m fig_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels_vgg\u001b[49m:\n\u001b[1;32m      5\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(f1_score(df_gray_del_lay[df_gray_del_lay[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mname][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m                             df_gray_del_lay[df_gray_del_lay[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mname][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_1\u001b[39m\u001b[38;5;124m\"\u001b[39m],average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m df_acc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: results},index \u001b[38;5;241m=\u001b[39m models_vgg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models_vgg' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "fig_width = 15\n",
    "results = []\n",
    "for name in models_vgg:\n",
    "    results.append(f1_score(df_gray_del_lay[df_gray_del_lay['model']==name][\"goal\"],\n",
    "                            df_gray_del_lay[df_gray_del_lay['model']==name][\"top_1\"],average='micro'))\n",
    "df_acc = pd.DataFrame({'': results},index = models_vgg)\n",
    "ax = df_acc.plot.bar(rot=30, figsize=(fig_width, fig_width//2), fontsize = 18)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "plt.legend(bbox_to_anchor=(1.15, .5), loc='right')\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
    "ax.set_ylabel('F1 Score', size= 20)\n",
    "ax.set_xlabel('Models', size= 20)\n",
    "plt.savefig('Cosyne_fig2.1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:21.816278Z",
     "iopub.status.busy": "2022-05-16T06:51:21.814695Z",
     "iopub.status.idle": "2022-05-16T06:51:21.819933Z",
     "shell.execute_reply": "2022-05-16T06:51:21.821215Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_acc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({model_name: {label: f1_score(df_[(df_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_1\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      2\u001b[0m                                                                df_[(df_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m                                                    average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m label, df_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m], [df_del_lay, df_gray_del_lay])} \n\u001b[1;32m      5\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models_vgg})\n\u001b[1;32m      7\u001b[0m ax \u001b[38;5;241m=\u001b[39m df_acc\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[1;32m      8\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_acc = pd.DataFrame({model_name: {label: f1_score(df_[(df_['model']==model_name)][\"top_1\"], \n",
    "                                                               df_[(df_['model']==model_name)][\"goal\"],\n",
    "                                                   average='micro')\n",
    "                                    for label, df_ in zip(['original', 'gray'], [df_del_lay, df_gray_del_lay])} \n",
    "                       for model_name in models_vgg})\n",
    "\n",
    "ax = df_acc.T.plot.bar(rot=30, figsize=(fig_width, fig_width//2), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.15, .5), loc='lower right')\n",
    "ax.grid(which='both', axis='y')\n",
    "for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "ax.set_title(f'Experiment 3 - color vs gray images', size=20)\n",
    "ax.set_ylabel('F1 Score', size=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation time\n",
    "Todo : plot time changer axe abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:21.849089Z",
     "iopub.status.busy": "2022-05-16T06:51:21.847514Z",
     "iopub.status.idle": "2022-05-16T06:51:21.853972Z",
     "shell.execute_reply": "2022-05-16T06:51:21.852671Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;28mlen\u001b[39m(models_vgg\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m*\u001b[39mphi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m color, df_, label, legend \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m], [df_gray_del_lay, df_del_lay], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGrayscale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegular\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax, model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axs, models_vgg\u001b[38;5;241m.\u001b[39mkeys()):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi/2))\n",
    "for color, df_, label, legend in zip(['gray', 'red'], [df_gray_del_lay, df_del_lay], ['black', 'color'], ['Grayscale', 'Regular']):\n",
    "    for ax, model_name in zip(axs, models_vgg.keys()):\n",
    "        ax.set_ylabel('Frequency', fontsize=20) \n",
    "        df_[df_['model']==model_name]['time'].plot.hist(bins=150, lw=1, label=str(legend+ ' ' + model_name), ax=ax, color=color, density=True)\n",
    "        ax.legend(loc='upper right', fontsize=20)\n",
    "        ax.set_xlim(df_gray_del_lay['time'].quantile(.01), df_gray_del_lay['time'].quantile(.99))\n",
    "        ax.legend(bbox_to_anchor=(1.18, .35), loc='lower right')\n",
    "axs[-1].set_xlabel('Processing time (s)', size=18)\n",
    "axs[0].set_title('Processed on : ' + args.HOST + '_' + str(df_del_lay['device_type'][0]), size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "While appliyng a grayscale transformation in the input we compare the performance with the ones from the [experiment 1](#exp1). Indeed all the networks seems robust to this transformation too. Some deepness even have a slightly better accuracy in this conditions (VGG-6, VGG-9 & VGG-10). As in the [experiment 2](#exp2) this may be explain by the complexity obn the features on which these networks rely to categorize the synset of interest in the scene. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Test on Serre_2007 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:21.860086Z",
     "iopub.status.busy": "2022-05-16T06:51:21.858867Z",
     "iopub.status.idle": "2022-05-16T06:51:21.861381Z",
     "shell.execute_reply": "2022-05-16T06:51:21.862289Z"
    }
   },
   "outputs": [],
   "source": [
    "scriptname = 'Prunning/experiment_serre.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:21.868699Z",
     "iopub.status.busy": "2022-05-16T06:51:21.867621Z",
     "iopub.status.idle": "2022-05-16T06:51:21.892419Z",
     "shell.execute_reply": "2022-05-16T06:51:21.893534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/experiment_serre.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "filename = f'results/{datetag}_results_del_serre_{HOST}.json'\n",
    "print(f'{filename=}')\n",
    "serre = ['targets', 'distractors']\n",
    "paths['animal']['test'] = '/home/INT/jeremie.j/Serre_2007'\n",
    "\n",
    "# Output's set up\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_serre = pd.read_json(filename)\n",
    "    else:\n",
    "        df_serre = pd.DataFrame([], columns=['model', 'model_task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "        i_trial = 0\n",
    "        # image preprocessing\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "        # Displays the input image of the model\n",
    "        for i_image, (data, label) in enumerate(dataloaders['animal']['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg:\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'targets' in image_datasets['animal']['test'].imgs[i_image][0] else 'distractor'\n",
    "                    model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0)\n",
    "                    likelihood = float((torch.sigmoid(out) * 100).detach().cpu().numpy()[0])\n",
    "                    top_1 = 'target' if likelihood>50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df_serre.loc[i_trial] = {'model':model_name, 'model_task':model_task, 'likelihood':likelihood, 'time':elapsed_time, \n",
    "                                         'fps': 1/elapsed_time, 'goal': goal, 'i_image':i_image, 'top_1':top_1,\n",
    "                                         'filename':image_datasets['animal']['test'].imgs[i_image][0], 'device':device.type}\n",
    "                print(f'The {model_name} model categorize {model_task} with {likelihood:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : animal, {goal}')\n",
    "                i_trial += 1\n",
    "    df_serre.to_json(filename)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:21.903383Z",
     "iopub.status.busy": "2022-05-16T06:51:21.901860Z",
     "iopub.status.idle": "2022-05-16T06:51:24.799182Z",
     "shell.execute_reply": "2022-05-16T06:51:24.800378Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_serre.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      5\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:24.816244Z",
     "iopub.status.busy": "2022-05-16T06:51:24.814684Z",
     "iopub.status.idle": "2022-05-16T06:51:24.819807Z",
     "shell.execute_reply": "2022-05-16T06:51:24.821043Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_del_serre_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m df_serre \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      3\u001b[0m df_serre\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOST' is not defined"
     ]
    }
   ],
   "source": [
    "filename = f'results/{datetag}_results_del_serre_{HOST}.json'\n",
    "df_serre = pd.read_json(filename)\n",
    "df_serre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:24.855616Z",
     "iopub.status.busy": "2022-05-16T06:51:24.854065Z",
     "iopub.status.idle": "2022-05-16T06:51:24.859277Z",
     "shell.execute_reply": "2022-05-16T06:51:24.860571Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, f1_score\n\u001b[0;32m----> 2\u001b[0m df_acc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: [accuracy_score(df_serre[df_serre[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_1\u001b[39m\u001b[38;5;124m\"\u001b[39m], df_serre[df_serre[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models_vgg\u001b[38;5;241m.\u001b[39mkeys()]}, index\u001b[38;5;241m=\u001b[39mmodels_vgg\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m ax \u001b[38;5;241m=\u001b[39m df_acc\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[1;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "df_acc = pd.DataFrame({'accuracy': [accuracy_score(df_serre[df_serre['model']==model_name][\"top_1\"], df_serre[df_serre['model']==model_name][\"goal\"]) for model_name in models_vgg.keys()]}, index=models_vgg.keys())\n",
    "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "ax.bar_label(ax.containers[0], padding=-24, color='black', fontsize=14, fmt='%.3f')\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.grid(which='both', axis='y')\n",
    "for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "ax.set_title('Average accuracy top_1 : for each models - Serre & al 2007 dataset', size=20)\n",
    "ax.set_xlabel('Model', size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo : image + time process + ccl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Image processing and recognition for differents flip transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:24.867536Z",
     "iopub.status.busy": "2022-05-16T06:51:24.865927Z",
     "iopub.status.idle": "2022-05-16T06:51:24.869846Z",
     "shell.execute_reply": "2022-05-16T06:51:24.868818Z"
    }
   },
   "outputs": [],
   "source": [
    "scriptname =  'experiment_flip_anim.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:24.876154Z",
     "iopub.status.busy": "2022-05-16T06:51:24.874860Z",
     "iopub.status.idle": "2022-05-16T06:51:24.897590Z",
     "shell.execute_reply": "2022-05-16T06:51:24.898759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment_flip_anim.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "datetag = '2022-01-11'\n",
    "flips = {}\n",
    "flips['Horizontal'] = transforms.RandomHorizontalFlip(p=1)\n",
    "flips['Vertical'] = transforms.RandomVerticalFlip(p=1)\n",
    "\n",
    "filename = f'results/{datetag}_results_flip_{args.HOST}.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_flip = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type', 'flip'])   \n",
    "        # image preprocessing\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "        for flip in flips:\n",
    "            pprint(flip)\n",
    "            task = 'animal'\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                data = flips[flip](data)\n",
    "                data, label, = data.to(device), label.to(device)\n",
    "                for model_name in models_vgg:\n",
    "                    model = models_vgg[model_name].to(device)\n",
    "                    with torch.no_grad():\n",
    "                        goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                        model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                        tic = time.time()\n",
    "                        out = model(data).squeeze(0)\n",
    "                        percentage = (torch.sigmoid(out) * 100).detach().cpu().numpy()[0]\n",
    "                        elapsed_time = time.time() - tic\n",
    "                        top_1 = 'target' if percentage>50 else 'distractor'\n",
    "                    df_flip.loc[i_trial] = {'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal, 'likelihood':percentage, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                           'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type, 'flip':flip}\n",
    "                    print(f'The {model_name} model categorize {model_task} with {percentage:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "                    i_trial += 1\n",
    "\n",
    "        df_flip.to_json(filename)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:24.909192Z",
     "iopub.status.busy": "2022-05-16T06:51:24.907646Z",
     "iopub.status.idle": "2022-05-16T06:51:27.865267Z",
     "shell.execute_reply": "2022-05-16T06:51:27.866131Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/experiment_flip_anim.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      5\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:27.881502Z",
     "iopub.status.busy": "2022-05-16T06:51:27.880141Z",
     "iopub.status.idle": "2022-05-16T06:51:27.884952Z",
     "shell.execute_reply": "2022-05-16T06:51:27.886183Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m datetag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-01-11\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_flip_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m df_flip \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      4\u001b[0m df_flip\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "datetag = '2022-01-11'\n",
    "filename = f'results/{datetag}_results_flip_{args.HOST}.json'\n",
    "df_flip = pd.read_json(filename)\n",
    "df_flip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the highest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:27.914003Z",
     "iopub.status.busy": "2022-05-16T06:51:27.912456Z",
     "iopub.status.idle": "2022-05-16T06:51:27.917617Z",
     "shell.execute_reply": "2022-05-16T06:51:27.918861Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_flip\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_flip.sort_values(by=['likelihood'], ascending=False).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_flip.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_flip.loc[idx]['top_1'] == df_flip.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_flip.loc[idx]['top_1'] + ' | ' + df_flip.loc[idx]['model'], color=color)\n",
    "    likelihood = df_flip.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the lowest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:27.945510Z",
     "iopub.status.busy": "2022-05-16T06:51:27.943959Z",
     "iopub.status.idle": "2022-05-16T06:51:27.949111Z",
     "shell.execute_reply": "2022-05-16T06:51:27.950355Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_flip\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_flip.sort_values(by=['likelihood'], ascending=True).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_flip.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_flip.loc[idx]['top_1'] == df_flip.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_flip.loc[idx]['top_1'] + ' | ' + df_flip.loc[idx]['model'], color=color)\n",
    "    likelihood = df_flip.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)  \n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:27.978807Z",
     "iopub.status.busy": "2022-05-16T06:51:27.977429Z",
     "iopub.status.idle": "2022-05-16T06:51:27.982062Z",
     "shell.execute_reply": "2022-05-16T06:51:27.983199Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#names_diff = ['vgg16_gen','vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8'] \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, f1_score\n\u001b[0;32m----> 4\u001b[0m df_acc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({model_name: {flip: accuracy_score(df_flip[(df_flip[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name) \u001b[38;5;241m&\u001b[39m (df_flip[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflip\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mflip)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_1\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      5\u001b[0m                                                                df_flip[(df_flip[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name) \u001b[38;5;241m&\u001b[39m (df_flip[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflip\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mflip)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m flip \u001b[38;5;129;01min\u001b[39;00m flips} \n\u001b[1;32m      7\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models_vgg})\n\u001b[1;32m      8\u001b[0m ax \u001b[38;5;241m=\u001b[39m df_acc\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      9\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "fig_width = 35\n",
    "#names_diff = ['vgg16_gen','vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8'] \n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "df_acc = pd.DataFrame({model_name: {flip: accuracy_score(df_flip[(df_flip['model']==model_name) & (df_flip['flip']==flip)][\"top_1\"], \n",
    "                                                               df_flip[(df_flip['model']==model_name) & (df_flip['flip']==flip)][\"goal\"])\n",
    "                                    for flip in flips} \n",
    "                       for model_name in models_vgg})\n",
    "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=30)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=18, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.05, .5), loc='center', fontsize=18)\n",
    "ax.set_ylabel('F1 Score', size=32)\n",
    "ax.set_xlabel('Flip', size=32)\n",
    "#plt.show();\n",
    "plt.savefig('del_lay_diff_size.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:28.009233Z",
     "iopub.status.busy": "2022-05-16T06:51:28.007649Z",
     "iopub.status.idle": "2022-05-16T06:51:28.012840Z",
     "shell.execute_reply": "2022-05-16T06:51:28.014091Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[43mdf_acc\u001b[49m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[1;32m      2\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mhlines(xmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.5\u001b[39m, xmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(models_vgg\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.5\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, ls\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, ec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchance level\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_acc' is not defined"
     ]
    }
   ],
   "source": [
    "ax = df_acc.T.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.grid(which='both', axis='y')\n",
    "for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "ax.set_title(f'Experiment 2 - image sizes = {args.image_sizes}', size=20)\n",
    "#ax.set_xticks([])\n",
    "ax.set_ylabel('F1 score', size=32)\n",
    "ax.set_xlabel('Models', size=32)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo : time process + ccl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment 5: Image processing and recognition on rotated images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:28.021034Z",
     "iopub.status.busy": "2022-05-16T06:51:28.019490Z",
     "iopub.status.idle": "2022-05-16T06:51:28.022647Z",
     "shell.execute_reply": "2022-05-16T06:51:28.023762Z"
    }
   },
   "outputs": [],
   "source": [
    "scriptname = 'experiment_rotate.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:28.030838Z",
     "iopub.status.busy": "2022-05-16T06:51:28.029571Z",
     "iopub.status.idle": "2022-05-16T06:51:28.052356Z",
     "shell.execute_reply": "2022-05-16T06:51:28.053461Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiment_rotate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "angles = np.arange(-180, 181)\n",
    "\n",
    "datetag = '2022-02-11'\n",
    "filename = f'results/{datetag}_results_rotate_prunning{args.HOST}.json'\n",
    "print(f'{filename=}')\n",
    "     \n",
    "\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_angle = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_mean = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=50)\n",
    "        # image preprocessing\n",
    "        #for task in args.tasks:\n",
    "        task = 'animal'\n",
    "        print(task)\n",
    "        for angle in angles:\n",
    "            print(f'Rotation by {angle=}°')\n",
    "            for model_name in models_vgg:\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                acc_= 0\n",
    "                for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                    data, label, = data.to(device), label.to(device)\n",
    "                    data = transforms.functional.rotate(data, angle=float(angle), expand = True)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(data)\n",
    "                        preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "                        acc_ += torch.sum(preds == label.data)\n",
    "                avg_acc = acc_ / dataset_sizes[task]['test']\n",
    "                df_mean.loc[i_trial] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':angle}  \n",
    "                print(model_name, avg_acc, angle)\n",
    "                #print(f'The {model_name} model categorize {model_task} with {likelihood:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "                torch.cuda.empty_cache()\n",
    "            df_mean.to_json(filename)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:28.063605Z",
     "iopub.status.busy": "2022-05-16T06:51:28.062083Z",
     "iopub.status.idle": "2022-05-16T06:51:30.969892Z",
     "shell.execute_reply": "2022-05-16T06:51:30.971052Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/experiment_rotate.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      5\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:30.987691Z",
     "iopub.status.busy": "2022-05-16T06:51:30.986180Z",
     "iopub.status.idle": "2022-05-16T06:51:30.991257Z",
     "shell.execute_reply": "2022-05-16T06:51:30.992505Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m datetag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-02-11\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_rotate_prunning\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m df_angle \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      4\u001b[0m df_angle\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "datetag = '2022-02-11'\n",
    "filename = f'results/{datetag}_results_rotate_prunning{args.HOST}.json'\n",
    "df_angle = pd.read_json(filename)\n",
    "df_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image display "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the highest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:31.021560Z",
     "iopub.status.busy": "2022-05-16T06:51:31.020040Z",
     "iopub.status.idle": "2022-05-16T06:51:31.025126Z",
     "shell.execute_reply": "2022-05-16T06:51:31.026346Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_angle\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_angle.sort_values(by=['likelihood'], ascending=False).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_angle.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_angle.loc[idx]['top_1'] == df_angle.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_angle.loc[idx]['top_1'] + ' | ' + df_angle.loc[idx]['model'] + ' | ' + str(df_angle.loc[idx]['angle']), color=color)\n",
    "    likelihood = df_angle.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the lowest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:31.055794Z",
     "iopub.status.busy": "2022-05-16T06:51:31.054263Z",
     "iopub.status.idle": "2022-05-16T06:51:31.060654Z",
     "shell.execute_reply": "2022-05-16T06:51:31.059340Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m N_image_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m N_image_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(N_image_i, N_image_j, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.3\u001b[39m, fig_width))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_image, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_angle\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(N_image_i\u001b[38;5;241m*\u001b[39mN_image_j)\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m      6\u001b[0m     ax \u001b[38;5;241m=\u001b[39m axs[i_image\u001b[38;5;241m%\u001b[39mN_image_i][i_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mN_image_i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_angle.sort_values(by=['likelihood'], ascending=True).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_datasets['animal']['test'].imgs[df_angle.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_angle.loc[idx]['top_1'] == df_angle.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_angle.loc[idx]['top_1'] + ' | ' + df_angle.loc[idx]['model'] + ' | ' + str(df_angle.loc[idx]['angle']), color=color)\n",
    "    likelihood = df_angle.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)  \n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:31.090942Z",
     "iopub.status.busy": "2022-05-16T06:51:31.089381Z",
     "iopub.status.idle": "2022-05-16T06:51:31.094516Z",
     "shell.execute_reply": "2022-05-16T06:51:31.095757Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, f1_score\n\u001b[0;32m----> 2\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      3\u001b[0m angles \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m181\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m cmap \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mget_cmap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis_r\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "angles = np.arange(-180, 181, 1)\n",
    "cmap = plt.cm.get_cmap('viridis_r')\n",
    "color_dict = pd.Series({k:cmap(i/len(df_angle['model'].unique())) for i,k in enumerate(df_angle['model'].unique())})\n",
    "for model_name in df_angle['model'].unique():     \n",
    "    df_angle[(df_angle['model']==model_name)].plot.scatter(x=\"var\", y=\"mean_prediction\", c=[color_dict[model_name],],\n",
    "                                                         marker='o', lw=0, label=model_name, ax=ax)\n",
    "ax.legend(loc='center right', fontsize = 12)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "#ax.set_ylim(0, 1)\n",
    "ax.set_xlim(-185, 185)\n",
    "#plt.set_cmap('Blues')\n",
    "ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "ax.set_ylabel('Mean Accuracies', size= 25, fontweight='bold')\n",
    "ax.set_xlabel('Rotation (°)', size= 25, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.11, .5), loc='center', fontsize=20)\n",
    "plt.tight_layout();\n",
    "plt.xticks(fontsize=20,  fontweight='bold')\n",
    "plt.yticks(fontsize=20,  fontweight='bold');\n",
    "plt.savefig('IJCNN_fig.7.b.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:31.102293Z",
     "iopub.status.busy": "2022-05-16T06:51:31.101175Z",
     "iopub.status.idle": "2022-05-16T06:51:31.141510Z",
     "shell.execute_reply": "2022-05-16T06:51:31.140182Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_angle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatistics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_angle\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m      3\u001b[0m     data_mean \u001b[38;5;241m=\u001b[39m df_angle\u001b[38;5;241m.\u001b[39mloc[df_angle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m name]\n\u001b[1;32m      4\u001b[0m     st_dev \u001b[38;5;241m=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mpstdev(data_mean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_angle' is not defined"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "for name in df_angle['model'].unique():\n",
    "    data_mean = df_angle.loc[df_angle['model'] == name]\n",
    "    st_dev = statistics.pstdev(data_mean['mean_prediction'])\n",
    "    mean =statistics.mean(data_mean['mean_prediction'])\n",
    "    print(\"Standard deviation of the given list: \" + name + '    ' + str(st_dev*100))\n",
    "    print(\"Mean of the given list: \" + name  + '    ' + str(mean*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:31.171536Z",
     "iopub.status.busy": "2022-05-16T06:51:31.169978Z",
     "iopub.status.idle": "2022-05-16T06:51:31.175132Z",
     "shell.execute_reply": "2022-05-16T06:51:31.176387Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, f1_score\n\u001b[0;32m----> 2\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      3\u001b[0m cmap \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mget_cmap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m angles \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m181\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "angles = np.arange(-180, 181, 1)\n",
    "color_dict = pd.Series({k:cmap(i/len(df_angle['model'].unique())) for i,k in enumerate(df_angle['model'].unique())})\n",
    "df_mean = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "model_name = 'vgg-4'     \n",
    "df_angle[(df_angle['model']==model_name)].plot.scatter(x=\"var\", y=\"mean_prediction\", c=[color_dict[model_name],],\n",
    "                                                   alpha=0.5, marker='o', lw=0, label=model_name, ax=ax)\n",
    "ax.legend(loc='center right', fontsize = 12)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlim(-185, 185)\n",
    "#plt.set_cmap('Blues')\n",
    "ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "ax.set_ylabel('Accuracy', size= 16)\n",
    "ax.set_xlabel('Rotation (°)', size= 16)\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='center', fontsize=14,)\n",
    "ax.set_title(' Rotation of the image from 0° angle to 360° angle around the center of the image', size = 20);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:31.202189Z",
     "iopub.status.busy": "2022-05-16T06:51:31.200802Z",
     "iopub.status.idle": "2022-05-16T06:51:31.205402Z",
     "shell.execute_reply": "2022-05-16T06:51:31.206497Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39mphi))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m color, model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(colors, models_vgg\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m      3\u001b[0m     axs \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mviolinplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mangle\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mdf_angle, inner\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquartile\u001b[39m\u001b[38;5;124m\"\u001b[39m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi))\n",
    "for color, model_name in zip(colors, models_vgg.keys()):\n",
    "    axs = sns.violinplot(x=\"angle\", y=\"time\", data=df_angle, inner=\"quartile\", hue='model')\n",
    "    axs.set_title('Processing time (s) for each network at different image size. Processed on : ' + args.HOST + '_' + str(df_downsample['device_type'][0]), size = 20)\n",
    "    axs.set_ylabel('Computation time (s)', size=18)\n",
    "    axs.set_xlabel('Image size', size=18)\n",
    "    axs.set_yscale('log')\n",
    "    axs.grid(which='both', axis='y')\n",
    "    for side in ['top', 'right'] :axs.spines[side].set_visible(False)\n",
    "h, l = axs.get_legend_handles_labels()\n",
    "axs.legend(h[:5], l[:5], loc='upper center', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo : time process + ccl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Image processing and recognition on shuffled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:31.212401Z",
     "iopub.status.busy": "2022-05-16T06:51:31.211262Z",
     "iopub.status.idle": "2022-05-16T06:51:31.213577Z",
     "shell.execute_reply": "2022-05-16T06:51:31.214414Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scriptname =  'Prunning/experiment_shuffle.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:31.220346Z",
     "iopub.status.busy": "2022-05-16T06:51:31.219157Z",
     "iopub.status.idle": "2022-05-16T06:51:31.242117Z",
     "shell.execute_reply": "2022-05-16T06:51:31.243279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/experiment_shuffle.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "datetag = '2022-02-09'\n",
    "filename = f'results/{datetag}_results_1_shuffle_{HOST}.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "task = 'animal'\n",
    "shuffle_sizes = [256, 128, 64, 32]\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_shuffle = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_shuffle = pd.DataFrame([], columns=['model', 'shuffle_size', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "            # image preprocessing\n",
    "        for shuffle_size in shuffle_sizes:\n",
    "            print(shuffle_size)\n",
    "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(\n",
    "                image_size=args.image_size, shuffle=shuffle_size, batch_size=1)\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                data, label = data.to(device), label.to(device)\n",
    "                for model_name in models_vgg:\n",
    "                    model = models_vgg[model_name].to(device)\n",
    "                    with torch.no_grad():\n",
    "                        goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                        tic = time.time()\n",
    "                        out = model(data).squeeze(0)\n",
    "                        percentage = float((torch.sigmoid(out) * 100).detach().cpu().numpy()[0])\n",
    "                        top_1 = 'target' if percentage>50 else 'distractor'\n",
    "                        elapsed_time = time.time() - tic\n",
    "                    df_shuffle.loc[i_trial] = {'model':model_name, 'shuffle_size':shuffle_size, 'top_1':top_1, 'goal':goal, 'likelihood':percentage, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                          'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type}\n",
    "                    #print(f'The {model_name} model categorize an animal with {percentage:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "                    i_trial += 1\n",
    "                    torch.cuda.empty_cache()\n",
    "    df_shuffle.to_json(filename)\n",
    "main()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:31.253518Z",
     "iopub.status.busy": "2022-05-16T06:51:31.252001Z",
     "iopub.status.idle": "2022-05-16T06:51:34.136282Z",
     "shell.execute_reply": "2022-05-16T06:51:34.137445Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_shuffle.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      6\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:34.154416Z",
     "iopub.status.busy": "2022-05-16T06:51:34.152906Z",
     "iopub.status.idle": "2022-05-16T06:51:34.157996Z",
     "shell.execute_reply": "2022-05-16T06:51:34.159227Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m datetag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-02-09\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_1_shuffle_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m df_shuffle \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      4\u001b[0m df_shuffle\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOST' is not defined"
     ]
    }
   ],
   "source": [
    "datetag = '2022-02-09'\n",
    "filename = f'results/{datetag}_results_1_shuffle_{HOST}.json'\n",
    "df_shuffle = pd.read_json(filename)\n",
    "df_shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:34.187385Z",
     "iopub.status.busy": "2022-05-16T06:51:34.185839Z",
     "iopub.status.idle": "2022-05-16T06:51:34.191038Z",
     "shell.execute_reply": "2022-05-16T06:51:34.192338Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fig_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m35\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, f1_score\n\u001b[0;32m----> 3\u001b[0m df_acc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({model_name: {shuffle_size: accuracy_score(df_shuffle[(df_shuffle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name) \u001b[38;5;241m&\u001b[39m (df_shuffle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mshuffle_size)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_1\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      4\u001b[0m                                                                df_shuffle[(df_shuffle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name) \u001b[38;5;241m&\u001b[39m (df_shuffle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mshuffle_size)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m shuffle_size \u001b[38;5;129;01min\u001b[39;00m shuffle_sizes} \n\u001b[1;32m      6\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models_vgg})\n\u001b[1;32m      8\u001b[0m ax \u001b[38;5;241m=\u001b[39m df_acc\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      9\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "fig_width = 35\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "df_acc = pd.DataFrame({model_name: {shuffle_size: accuracy_score(df_shuffle[(df_shuffle['model']==model_name) & (df_shuffle['shuffle_size']==shuffle_size)][\"top_1\"], \n",
    "                                                               df_shuffle[(df_shuffle['model']==model_name) & (df_shuffle['shuffle_size']==shuffle_size)][\"goal\"])\n",
    "                                    for shuffle_size in shuffle_sizes} \n",
    "                       for model_name in models_vgg})\n",
    "\n",
    "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=30)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=18, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.05, .5), loc='center', fontsize=18)\n",
    "#ax.set_title(f'Experiment 2 - image sizes = {args.image_sizes}', size=20)\n",
    "ax.set_ylabel('F1 Score', size=32)\n",
    "ax.set_xlabel('Shuffle size', size=32)\n",
    "#plt.show();\n",
    "#plt.savefig('del_lay_diff_size.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:34.220522Z",
     "iopub.status.busy": "2022-05-16T06:51:34.218955Z",
     "iopub.status.idle": "2022-05-16T06:51:34.224104Z",
     "shell.execute_reply": "2022-05-16T06:51:34.225266Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cmap \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mget_cmap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m color_dict \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({k:cmap(i\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(df_acc)) \u001b[38;5;28;01mfor\u001b[39;00m i,k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_acc)})\n\u001b[1;32m      3\u001b[0m ax \u001b[38;5;241m=\u001b[39m df_acc\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1.2\u001b[39m, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m6\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22\u001b[39m, color\u001b[38;5;241m=\u001b[39mcolor_dict)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "color_dict = pd.Series({k:cmap(i/len(df_acc)) for i,k in enumerate(df_acc)})\n",
    "ax = df_acc.T.plot.bar(rot=0, figsize=(fig_width/1.2, fig_width//6), fontsize=22, color=color_dict)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/2, ls='--', ec='k')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='white', fontsize=14, fmt='%.3f', rotation=90, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, .3), fontsize=16, loc='lower right')\n",
    "ax.grid(which='both', axis='y')\n",
    "for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "#ax.set_xticks([])\n",
    "plt.xticks(fontsize=20,  fontweight='bold')\n",
    "plt.yticks(fontsize=20,  fontweight='bold')\n",
    "plt.tight_layout();\n",
    "plt.savefig('IJCNN_fig.7.c.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:34.264062Z",
     "iopub.status.busy": "2022-05-16T06:51:34.262458Z",
     "iopub.status.idle": "2022-05-16T06:51:34.268922Z",
     "shell.execute_reply": "2022-05-16T06:51:34.267618Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, f1_score\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#shuffle_sizes = [32, 64, 128, 256]\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      4\u001b[0m cmap \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mget_cmap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m color_dict \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({k:cmap(i\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(df_shuffle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())) \u001b[38;5;28;01mfor\u001b[39;00m i,k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_shuffle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "#shuffle_sizes = [32, 64, 128, 256]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "color_dict = pd.Series({k:cmap(i/len(df_shuffle['model'].unique())) for i,k in enumerate(df_shuffle['model'].unique())})\n",
    "df_mean = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "diff_ = np.arange(0, 28, 1)\n",
    "shuffle_sizes = [256, 128, 64, 32]\n",
    "for i, model_name in enumerate(df_shuffle['model'].unique()):\n",
    "    df_mean = pd.DataFrame([], columns=[' ', 'model', '  '])\n",
    "    for j, shuffle_size in enumerate(shuffle_sizes):\n",
    "        if i%2 ==1:\n",
    "            diff_[i]=-diff_[i]\n",
    "        accuracy = accuracy_score(df_shuffle[(df_shuffle['model']==model_name) & (df_shuffle['shuffle_size']==shuffle_size)][\"top_1\"], \n",
    "                                                               df_shuffle[(df_shuffle['model']==model_name) & (df_shuffle['shuffle_size']==shuffle_size)][\"goal\"])\n",
    "        df_mean.loc[j] = {'model':model_name, ' ':accuracy, '  ':shuffle_size - diff_[i]}      \n",
    "    df_mean[(df_mean['model']==model_name)].plot.scatter(x=\"  \", y=\" \", c=[color_dict[model_name],],\n",
    "                                                         alpha=0.9, marker=(i+2, 1, 0), s=350, lw=0, label=model_name, ax=ax) # marker=(i, style, angle), s=50\n",
    "ax.legend(loc='center right', fontsize = 12)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.hlines(xmin=0, xmax=shuffle_sizes[0]+diff_[-1], y=1/2, ls='--', ec='k', label='chance level')\n",
    "ax.set_xticks(shuffle_sizes)\n",
    "ax.set_ylim(0.45, 1)\n",
    "#ax.set_xscale(\"log\")\n",
    "ax.set_ylabel('Accuracy', size= 24, fontweight='bold' )\n",
    "plt.legend(bbox_to_anchor=(1.13, .5), loc='center', fontsize=18)\n",
    "plt.xticks(fontsize=20,  fontweight='bold')\n",
    "plt.yticks(fontsize=20,  fontweight='bold')\n",
    "plt.xticks(shuffle_sizes)\n",
    "plt.tight_layout()\n",
    "plt.savefig('IJCNN_fig.10.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo : Image + time + remake last accuracy graph + CCl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation on the performances with different synsets complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:34.275197Z",
     "iopub.status.busy": "2022-05-16T06:51:34.273992Z",
     "iopub.status.idle": "2022-05-16T06:51:34.276504Z",
     "shell.execute_reply": "2022-05-16T06:51:34.277408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scriptname =  'Prunning/experiment_cat.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:34.284240Z",
     "iopub.status.busy": "2022-05-16T06:51:34.282986Z",
     "iopub.status.idle": "2022-05-16T06:51:34.307074Z",
     "shell.execute_reply": "2022-05-16T06:51:34.308204Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/experiment_cat.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "filename = f'results/{datetag}_results_1_cat_{HOST}.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "task = 'cat'\n",
    "\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_cat = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_cat = pd.DataFrame([], columns=['model', 'model_task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "            # image preprocessing\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg:\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0)\n",
    "                    percentage = float((torch.sigmoid(out) * 100).detach().cpu().numpy()[0])\n",
    "                    top_1 = 'target' if percentage>50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df_cat.loc[i_trial] = {'model':model_name, 'model_task':task, 'top_1':top_1, 'goal':goal, 'likelihood':percentage, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                      'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type}\n",
    "                print(f'The {model_name} model categorize an animal with {percentage:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "    df_cat.to_json(filename)\n",
    "main()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:34.318300Z",
     "iopub.status.busy": "2022-05-16T06:51:34.316802Z",
     "iopub.status.idle": "2022-05-16T06:51:37.290218Z",
     "shell.execute_reply": "2022-05-16T06:51:37.291387Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_cat.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      6\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:37.307855Z",
     "iopub.status.busy": "2022-05-16T06:51:37.306317Z",
     "iopub.status.idle": "2022-05-16T06:51:37.311420Z",
     "shell.execute_reply": "2022-05-16T06:51:37.312664Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_1_cat_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m df_cat \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      3\u001b[0m df_cat\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOST' is not defined"
     ]
    }
   ],
   "source": [
    "filename = f'results/{datetag}_results_1_cat_{HOST}.json'\n",
    "df_cat = pd.read_json(filename)\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:37.318982Z",
     "iopub.status.busy": "2022-05-16T06:51:37.317726Z",
     "iopub.status.idle": "2022-05-16T06:51:37.320295Z",
     "shell.execute_reply": "2022-05-16T06:51:37.321214Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scriptname =  'Prunning/experiment_dog.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:37.327625Z",
     "iopub.status.busy": "2022-05-16T06:51:37.326371Z",
     "iopub.status.idle": "2022-05-16T06:51:37.348848Z",
     "shell.execute_reply": "2022-05-16T06:51:37.349951Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/experiment_dog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "filename = f'results/{datetag}_results_1_dog_{HOST}.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "task = 'dog'\n",
    "\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_dog = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_dog = pd.DataFrame([], columns=['model', 'model_task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "            # image preprocessing\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg:\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0)\n",
    "                    percentage = float((torch.sigmoid(out) * 100).detach().cpu().numpy()[0])\n",
    "                    top_1 = 'target' if percentage>50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df_dog.loc[i_trial] = {'model':model_name, 'model_task':task, 'top_1':top_1, 'goal':goal, 'likelihood':percentage, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                      'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type}\n",
    "                print(f'The {model_name} model categorize an animal with {percentage:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "    df_dog.to_json(filename)\n",
    "main()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:37.360085Z",
     "iopub.status.busy": "2022-05-16T06:51:37.358550Z",
     "iopub.status.idle": "2022-05-16T06:51:40.274223Z",
     "shell.execute_reply": "2022-05-16T06:51:40.275382Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_dog.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      6\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:40.292889Z",
     "iopub.status.busy": "2022-05-16T06:51:40.291346Z",
     "iopub.status.idle": "2022-05-16T06:51:40.296467Z",
     "shell.execute_reply": "2022-05-16T06:51:40.297699Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m datetag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-01-31\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_1_dog_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m df_dog \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      4\u001b[0m df_dog\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOST' is not defined"
     ]
    }
   ],
   "source": [
    "datetag = '2022-01-31'\n",
    "filename = f'results/{datetag}_results_1_dog_{HOST}.json'\n",
    "df_dog = pd.read_json(filename)\n",
    "df_dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:40.303776Z",
     "iopub.status.busy": "2022-05-16T06:51:40.302552Z",
     "iopub.status.idle": "2022-05-16T06:51:40.305094Z",
     "shell.execute_reply": "2022-05-16T06:51:40.306014Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scriptname =  'Prunning/experiment_bird.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:40.312576Z",
     "iopub.status.busy": "2022-05-16T06:51:40.311404Z",
     "iopub.status.idle": "2022-05-16T06:51:40.335305Z",
     "shell.execute_reply": "2022-05-16T06:51:40.336428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/experiment_bird.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "\n",
    "from Prunning.experiment_train import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "filename = f'results/{datetag}_results_1_bird_{HOST}.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "task = 'bird'\n",
    "\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_bird = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_bird = pd.DataFrame([], columns=['model', 'model_task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "            # image preprocessing\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg:\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0)\n",
    "                    percentage = float((torch.sigmoid(out) * 100).detach().cpu().numpy()[0])\n",
    "                    top_1 = 'target' if percentage>50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df_bird.loc[i_trial] = {'model':model_name, 'model_task':task, 'top_1':top_1, 'goal':goal, 'likelihood':percentage, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                      'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type}\n",
    "                print(f'The {model_name} model categorize an animal with {percentage:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "    df_bird.to_json(filename)\n",
    "main()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:40.346629Z",
     "iopub.status.busy": "2022-05-16T06:51:40.345120Z",
     "iopub.status.idle": "2022-05-16T06:51:43.188918Z",
     "shell.execute_reply": "2022-05-16T06:51:43.190078Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_bird.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      6\u001b[0m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/experiment_train.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPrunning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, num_epochs, dataloaders, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum, beta2\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta2, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:43.207755Z",
     "iopub.status.busy": "2022-05-16T06:51:43.206234Z",
     "iopub.status.idle": "2022-05-16T06:51:43.211347Z",
     "shell.execute_reply": "2022-05-16T06:51:43.212505Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_1_bird_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m df_bird \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      3\u001b[0m df_bird\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOST' is not defined"
     ]
    }
   ],
   "source": [
    "filename = f'results/{datetag}_results_1_bird_{HOST}.json'\n",
    "df_bird = pd.read_json(filename)\n",
    "df_bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between different synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:43.244044Z",
     "iopub.status.busy": "2022-05-16T06:51:43.242461Z",
     "iopub.status.idle": "2022-05-16T06:51:43.247690Z",
     "shell.execute_reply": "2022-05-16T06:51:43.248994Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, f1_score\n\u001b[0;32m----> 2\u001b[0m df_acc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({model_name: {label: f1_score(df_[(df_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_1\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      3\u001b[0m                                                                df_[(df_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m                                                    average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m label, df_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manimal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbird\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m], [df_del_lay, df_bird, df_dog, df_cat,])} \n\u001b[1;32m      6\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models_vgg})\n\u001b[1;32m      8\u001b[0m ax \u001b[38;5;241m=\u001b[39m df_acc\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[1;32m      9\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "df_acc = pd.DataFrame({model_name: {label: f1_score(df_[(df_['model']==model_name)][\"top_1\"], \n",
    "                                                               df_[(df_['model']==model_name)][\"goal\"],\n",
    "                                                   average='micro')\n",
    "                                    for label, df_ in zip(['animal', 'bird', 'dog', 'cat'], [df_del_lay, df_bird, df_dog, df_cat,])} \n",
    "                       for model_name in models_vgg})\n",
    "\n",
    "ax = df_acc.T.plot.bar(rot=30, figsize=(fig_width, fig_width//2), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.15, .5), loc='lower right')\n",
    "ax.grid(which='both', axis='y')\n",
    "for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "ax.set_title(f'Experiment 3 - color vs gray images', size=20)\n",
    "ax.set_ylabel('F1 Score', size=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo : time + CCl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus : Why can't we use the the pre-trained VGG network trained on the [Imagenet](http://image-net.org/) dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:43.255523Z",
     "iopub.status.busy": "2022-05-16T06:51:43.254115Z",
     "iopub.status.idle": "2022-05-16T06:51:43.256833Z",
     "shell.execute_reply": "2022-05-16T06:51:43.257752Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scriptname =  'Prunning/bonus_vgg16.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:43.265339Z",
     "iopub.status.busy": "2022-05-16T06:51:43.264001Z",
     "iopub.status.idle": "2022-05-16T06:51:43.287790Z",
     "shell.execute_reply": "2022-05-16T06:51:43.288918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Prunning/bonus_vgg16.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "\n",
    "from  src.model import *\n",
    "from PIL import ImageFile\n",
    "import math\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "filename = f'results/{datetag}_results_bonus_vgg16_{HOST}.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "task = 'animal'\n",
    "\n",
    "match = []\n",
    "class_loader = 'imagenet_label_to_wordnet_synset.json'\n",
    "with open(class_loader, 'r') as fp: \n",
    "    imagenet = json.load(fp)\n",
    "for a, img_id in enumerate(imagenet):\n",
    "    labels.append(imagenet[img_id]['label'])\n",
    "    syn_ = wn.synset_from_pos_and_offset('n', int(imagenet[img_id]['id'].replace('-n','')))\n",
    "    sem_ = syn_.hypernym_paths()[0]\n",
    "    for i in np.arange(len(sem_)):\n",
    "        if task in sem_[i].lemmas()[0].name() :\n",
    "            match.append(a)\n",
    "models_vgg = {}    \n",
    "model_filenames = {}    \n",
    "models_vgg['VGG_Gen'] = torchvision.models.vgg16(pretrained=True)\n",
    "m = 0\n",
    "input_lin_1 = 12544\n",
    "vgg_head = torchvision.models.vgg16(pretrained=True)\n",
    "all_models = ['vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8','vgg-9','vgg-10', 'vgg-11', 'vgg-12']\n",
    "goal =  list(vgg_head.features.children())\n",
    "for model_name in all_models:\n",
    "    model_filenames[model_name] = str(args.model_path +model_name+'.pt')\n",
    "    models_vgg[model_name] = torchvision.models.vgg16(pretrained=True)\n",
    "    if len(goal) > 28 :\n",
    "        del goal[24:26] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal) #get the new features parameters in the model\n",
    "    elif len(goal) > 26:\n",
    "        del goal[24:26] # remove the three last Conv2d and ReLU layers\n",
    "        del goal[-1] # remove the last MaxPool2d layer when no Conv2d or ReLU layer left\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal) \n",
    "    elif len(goal) > 20: # same as above for the next three Conv2d and ReLU layers\n",
    "        del goal[19:21] \n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal) \n",
    "    elif len(goal) > 17:\n",
    "        del goal[17:19] # remove the three last Conv2d and ReLU layers\n",
    "        del goal[-1] # remove the last MaxPool2d layer when no Conv2d or ReLU layer left\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    elif len(goal) > 13:\n",
    "        del goal[11:13] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    elif len(goal) > 10:\n",
    "        del goal[10:12] # remove the three last Conv2d and ReLU layers\n",
    "        del goal[-1] # remove the last MaxPool2d layer when no Conv2d or ReLU layer left\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    elif len(goal)>9:\n",
    "        del goal[6:8] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    elif len(goal)>7:\n",
    "        del goal[3:6] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "    else:\n",
    "        del goal[2:4] # remove the three last Conv2d and ReLU layers\n",
    "        models_vgg[model_name].features = torch.nn.Sequential(*goal)\n",
    "        \n",
    "        \n",
    "    if model_name in ['vgg-6','vgg-7','vgg-8']:   \n",
    "        num_features = models_vgg[model_name].classifier[6].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children())\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1, num_features)) # Add your custom input layer\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "        \n",
    "    elif model_name in ['vgg-9','vgg-10']:\n",
    "        num_features = models_vgg[model_name].classifier[6].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children()) # Remove last layer\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1//2, num_features)) # Add your custom input layer\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "        \n",
    "    elif model_name in ['vgg-11','vgg-12']:\n",
    "        num_features = models_vgg[model_name].classifier[6].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children()) # Remove last layer\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1//4, num_features)) # Add your custom input layer\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "\n",
    "    else :     \n",
    "        m += 1\n",
    "        \n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        test = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        test = pd.DataFrame([], columns=['model', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "            # image preprocessing\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "        pprint('GO!')\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name  in models_vgg:\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                temp_, likelihood = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0)\n",
    "                    percentage = torch.nn.functional.softmax(out, dim=0) * 100\n",
    "                    for idx in np.arange(0,1000,1):\n",
    "                        if idx in match:\n",
    "                            likelihood += math.exp(percentage[idx].item())\n",
    "                        else:\n",
    "                            temp_ += math.exp(percentage[idx].item())  \n",
    "                    likelihood = likelihood/temp_\n",
    "                    top_1= 'target' if likelihood>50 else 'distractor' \n",
    "                    elapsed_time = time.time() - tic\n",
    "                test.loc[i_trial] = {'model':model_name, 'likelihood':likelihood, 'top_1':top_1, 'goal':goal, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                      'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type}\n",
    "                print(f'The {model_name} model categorize an animal with {likelihood:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtrue : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "    test.to_json(filename)\n",
    "main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:43.298633Z",
     "iopub.status.busy": "2022-05-16T06:51:43.297108Z",
     "iopub.status.idle": "2022-05-16T06:51:46.192573Z",
     "shell.execute_reply": "2022-05-16T06:51:46.193747Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-16_config_args.json\n",
      "On date 2022-05-16 , Running benchmark on host neo-ope-de04  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/Prunning/bonus_vgg16.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import model's script and set the output file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m  \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m/hpc/neopto/model/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/INT/perrinet.l/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:46.211634Z",
     "iopub.status.busy": "2022-05-16T06:51:46.210125Z",
     "iopub.status.idle": "2022-05-16T06:51:46.215208Z",
     "shell.execute_reply": "2022-05-16T06:51:46.216443Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_bonus_vgg16_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m results_bonus_vgg16 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(filename)\n\u001b[1;32m      3\u001b[0m results_bonus_vgg16\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOST' is not defined"
     ]
    }
   ],
   "source": [
    "filename = f'results/{datetag}_results_bonus_vgg16_{HOST}.json'\n",
    "results_bonus_vgg16 = pd.read_json(filename)\n",
    "results_bonus_vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T06:51:46.246687Z",
     "iopub.status.busy": "2022-05-16T06:51:46.245126Z",
     "iopub.status.idle": "2022-05-16T06:51:46.250335Z",
     "shell.execute_reply": "2022-05-16T06:51:46.251609Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_acc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({model_name: {label: f1_score(df_[(df_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_1\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      2\u001b[0m                                                                df_[(df_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mmodel_name)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m                                                    average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m label, df_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre-train\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m'\u001b[39m], [df_del_lay, results_bonus_vgg16])} \n\u001b[1;32m      5\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models_vgg})\n\u001b[1;32m      7\u001b[0m ax \u001b[38;5;241m=\u001b[39m df_acc\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(fig_width, fig_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[1;32m      8\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_acc = pd.DataFrame({model_name: {label: f1_score(df_[(df_['model']==model_name)][\"top_1\"], \n",
    "                                                               df_[(df_['model']==model_name)][\"goal\"],\n",
    "                                                   average='micro')\n",
    "                                    for label, df_ in zip(['re-train', 'pytorch'], [df_del_lay, results_bonus_vgg16])} \n",
    "                       for model_name in models_vgg})\n",
    "\n",
    "ax = df_acc.T.plot.bar(rot=30, figsize=(fig_width, fig_width//2), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.15, .5), loc='lower right')\n",
    "ax.grid(which='both', axis='y')\n",
    "for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "ax.set_title(f'Experiment 3 - color vs gray images', size=20)\n",
    "ax.set_ylabel('F1 Score', size=14)\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
