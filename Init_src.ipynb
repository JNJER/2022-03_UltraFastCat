{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adfaf06",
   "metadata": {},
   "source": [
    "# Ultra-fast categorization of image containing animals *in computo*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78382ad5-66d9-45fc-b7e5-4a7b9f13d5da",
   "metadata": {},
   "source": [
    "Hi! I am  [Jean-Nicolas Jérémie](https://github.com/JNJER) in this notebook, I will use the [Pytorch](https://pytorch.org/) library for running the networks and the [pandas](https://pandas.pydata.org/docs/getting_started/index.html) library to collect and display the results. This notebook focused on the training and the test of Deep convolutional neuronal networks (DCNNs) for ecological tasks such as detecting an animal in a natural scene.\n",
    "\n",
    "Our coding strategy is to build up a small library as a package of scripts in the `src` folder and to run all calls to that library from this notebook. This notebook contains the scripts necessary to set the variables of the definition and the training of the networks (`init.py` & `model.py`). Thus this notebook allow the generation of dataset based on the [Imagenet database](https://www.image-net.org/) dedicates to a specific ecologic task (`dataset.py`). \n",
    "\n",
    "The set of labels of the Imagenet database is based on a large lexical database of English: [Wordnet](https://wordnet.princeton.edu/) . The nouns, verbs, adjectives, and adverbs in this database are grouped into a graphical set of cognitive synonyms (synset), each expressing a distinct concept. These synsets are linked between them by employing a small number of conceptual relations. I used the hyperonym link, for instance, a German shepherd is kind of a dog and a dog is a kind of an animal thus defining an hyperonym path. So the synset 'animal' is in the hyperonym path of the synset 'German sheperd'. Based on this relation, we selected a specific subset of labels in the Imagenet database to build our datasets.\n",
    "\n",
    "This notebook was done during my thesis at the Neurosciences Institute of Timone (INT) under the supervision of [Laurent PERRINET](https://laurentperrinet.github.io/). It is curated in the following [github repo](https://github.com/JNJER/2022-03_UltraFastCat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645d51cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:04.318096Z",
     "iopub.status.busy": "2022-05-13T16:43:04.317676Z",
     "iopub.status.idle": "2022-05-13T16:43:05.320618Z",
     "shell.execute_reply": "2022-05-13T16:43:05.319665Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%mkdir -p src\n",
    "%mkdir -p results\n",
    "%mkdir -p models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4fc8f",
   "metadata": {},
   "source": [
    "## Initialization of the libraries/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2492bf",
   "metadata": {},
   "source": [
    "First of all, a `src/init.py` to define all our usefull variables like the new labels to learn, the number of training images or the root. Also, we importe libraries to train the networks and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe0c067d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:05.325249Z",
     "iopub.status.busy": "2022-05-13T16:43:05.324945Z",
     "iopub.status.idle": "2022-05-13T16:43:05.893144Z",
     "shell.execute_reply": "2022-05-13T16:43:05.892521Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/laurent/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "scriptname = 'src/init.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f1b5e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:05.899156Z",
     "iopub.status.busy": "2022-05-13T16:43:05.898974Z",
     "iopub.status.idle": "2022-05-13T16:43:05.907262Z",
     "shell.execute_reply": "2022-05-13T16:43:05.906507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "# Importing libraries\n",
    "import torch\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('xtick', labelsize=18)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=18)    # fontsize of the tick labels\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "#from numpy import random\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from time import strftime, gmtime\n",
    "datetag = strftime(\"%Y-%m-%d\", gmtime())\n",
    "\n",
    "HOST, device = os.uname()[1], torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "# to store results\n",
    "import pandas as pd\n",
    "\n",
    "def arg_parse():\n",
    "    #DEBUG = 25\n",
    "    DEBUG = 1\n",
    "    parser = argparse.ArgumentParser(description='DCNN_transfer_learning/init.py set root')\n",
    "    parser.add_argument(\"--root\", dest = 'root', help = \"Directory containing images to perform the training\",\n",
    "                        default = '../data', type = str)\n",
    "    parser.add_argument(\"--folders\", dest = 'folders', help =  \"Set the training, validation and testing folders relative to the root\",\n",
    "                        default = ['test', 'val', 'train'], type = list)\n",
    "    parser.add_argument(\"--tasks\", dest = 'tasks', help =  \"Set the training, validation and testing folders relative to the root\",\n",
    "                        default = ['animal'], type = list)\n",
    "    parser.add_argument(\"--goals\", dest = 'goals', help =  \"Set the training, validation and testing folders relative to the root\",\n",
    "                        default = ['target', 'distractor'], type = list)\n",
    "    parser.add_argument(\"--N_images\", dest = 'N_images', help =\"Set the number of images per classe in the train folder\",\n",
    "                        default = [600//DEBUG, 400//DEBUG, 1000//DEBUG], type = list)\n",
    "    parser.add_argument(\"--HOST\", dest = 'HOST', help = \"Set the name of your machine\",\n",
    "                    default=HOST, type = str)\n",
    "    parser.add_argument(\"--datetag\", dest = 'datetag', help = \"Set the datetag of the result's file\",\n",
    "                    default = datetag, type = str)\n",
    "    parser.add_argument(\"--image_size\", dest = 'image_size', help = \"Set the default image_size of the input\",\n",
    "                    default = 256)\n",
    "    parser.add_argument(\"--image_sizes\", dest = 'image_sizes', help = \"Set the image_sizes of the input for experiment 2 (downscaling)\",\n",
    "                    default = [64, 128, 256, 512], type = list)\n",
    "    parser.add_argument(\"--num_epochs\", dest = 'num_epochs', help = \"Set the number of epoch to perform during the traitransportationning phase\",\n",
    "                    default = 25//DEBUG)\n",
    "    parser.add_argument(\"--batch_size\", dest = 'batch_size', help=\"Set the batch size\", default = 32)\n",
    "    parser.add_argument(\"--lr\", dest = 'lr', help=\"Set the learning rate\", default = 0.0001)\n",
    "    parser.add_argument(\"--momentum\", dest = 'momentum', help=\"Set the momentum\", default = 0.9)\n",
    "    parser.add_argument(\"--beta2\", dest = 'beta2', help=\"Set the second momentum - use zero for SGD\", default = 0.)\n",
    "    parser.add_argument(\"--url_loader\", dest = 'url_loader', help = \"Set the file containing imagenet urls\",\n",
    "                        default = 'Imagenet_urls_ILSVRC_2016.json', type = str)\n",
    "    parser.add_argument(\"--model_path\", dest = 'model_path', help = \"Set the path to the pre-trained model\",\n",
    "                        default = 'models/re-trained_', type = str)\n",
    "    parser.add_argument(\"--model_names\", dest = 'model_names', help = \"Modes for the new trained networks\",\n",
    "                        default = ['vgg16_gen', 'vgg16_scale', 'vgg16_gray', ], type = list)\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = arg_parse()\n",
    "datetag = args.datetag\n",
    "json_fname = os.path.join('results', datetag + '_config_args.json')\n",
    "load_parse = False # False to custom the config\n",
    "\n",
    "if load_parse:\n",
    "    with open(json_fname, 'rt') as f:\n",
    "        print(f'file {json_fname} exists: LOADING')\n",
    "        override = json.load(f)\n",
    "        args.__dict__.update(override)\n",
    "else:\n",
    "    print(f'Creating file {json_fname}')\n",
    "    with open(json_fname, 'wt') as f:\n",
    "        json.dump(vars(args), f, indent=4)\n",
    "    \n",
    "# matplotlib parameters\n",
    "colors = ['b', 'r', 'k', 'g', 'm', 'y']\n",
    "fig_width = 20\n",
    "phi = (np.sqrt(5)+1)/2 # golden ratio for the figures :-)\n",
    "\n",
    "#to plot & display \n",
    "def pprint(message): #display function\n",
    "    print('-'*len(message))\n",
    "    print(message)\n",
    "    print('-'*len(message))\n",
    "    \n",
    "#DCCN training\n",
    "print('On date', args.datetag, ', Running benchmark on host', args.HOST, ' with device', device.type)\n",
    "\n",
    "paths_task ={}\n",
    "paths = {}\n",
    "class_wnids = {}\n",
    "N_images_per_class = {}\n",
    "match = {}\n",
    "\n",
    "for task in args.tasks :\n",
    "    paths_task[task] = os.path.join(args.root, task) # data path\n",
    "    os.makedirs(paths_task[task], exist_ok=True)\n",
    "    paths[task] = {}\n",
    "    match[task] = []\n",
    "    for folder, N_image in zip(args.folders, args.N_images):\n",
    "        paths[task][folder] = os.path.join(args.root, task, folder) # data path\n",
    "        N_images_per_class[folder] = N_image\n",
    "        os.makedirs(paths[task][folder], exist_ok=True)\n",
    "        class_wnids[str(task)] = {}\n",
    "        for goal in args.goals:\n",
    "            class_wnids[str(task)][str(goal)] = []\n",
    "    \n",
    "with open(args.url_loader) as json_file:\n",
    "    Imagenet_urls_ILSVRC_2016 = json.load(json_file)\n",
    "\n",
    "# gathering labels\n",
    "labels = []\n",
    "reverse_id_labels = {}\n",
    "for i_img, img_id in enumerate(Imagenet_urls_ILSVRC_2016):\n",
    "    syn_= wn.synset_from_pos_and_offset('n', int(img_id.replace('n','')))\n",
    "    reverse_id_labels[img_id] = syn_.lemmas()[0].name()\n",
    "    labels.append(syn_.lemmas()[0].name())\n",
    "    sem_ = syn_.hypernym_paths()[0]\n",
    "    for task in args.tasks :\n",
    "        for i in np.arange(len(sem_)):\n",
    "            if sem_[i].lemmas()[0].name() == str(task) :\n",
    "                class_wnids[str(task)]['target'].append(img_id)\n",
    "                match[task].append(i_img)\n",
    "        if img_id not in class_wnids[str(task)]['target']:\n",
    "            class_wnids[str(task)]['distractor'].append(img_id)\n",
    "\n",
    "        \n",
    "# a reverse look-up-table giving the index of a given label (within the whole set of imagenet labels)\n",
    "reverse_labels = {}\n",
    "for i_label, label in enumerate(labels):\n",
    "    reverse_labels[label] = i_label\n",
    "\n",
    "# a reverse look-up-table giving the label of a given index in the last layer of the new model (within the sub-set of classes)\n",
    "reverse_model_labels = args.tasks\n",
    "reverse_model_labels.sort()\n",
    "\n",
    "pprint('List of Pre-selected tasks : ')\n",
    "# choosing the selected classes for recognition\n",
    "for task in args.tasks :\n",
    "    print(len(class_wnids[str(task)]['target']), ' Targets for the task :', task)\n",
    "    print(len(class_wnids[str(task)]['distractor']), ' Distractors for the task :', task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e137aff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:05.910192Z",
     "iopub.status.busy": "2022-05-13T16:43:05.910003Z",
     "iopub.status.idle": "2022-05-13T16:43:08.045397Z",
     "shell.execute_reply": "2022-05-13T16:43:08.044797Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-13_config_args.json\n",
      "On date 2022-05-13 , Running benchmark on host babbage  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/home/laurent/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/home/laurent/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/metagit/JNJER/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovenances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momw_prov\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m provdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1284\u001b[0m provdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1285\u001b[0m fileids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_omw_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m()\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m   1287\u001b[0m     prov, langfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(fileid)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/home/laurent/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run src/init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8423c",
   "metadata": {},
   "source": [
    "## Download the `train` & `val` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73e4b5-b648-4626-bb79-ac1644afec5c",
   "metadata": {},
   "source": [
    "In the dataset.py, we use an archive of the Imagenet fall 2011 urls to populate datasets based on the pre-selected synsets listed in the `src/init.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea089d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:08.049571Z",
     "iopub.status.busy": "2022-05-13T16:43:08.049222Z",
     "iopub.status.idle": "2022-05-13T16:43:08.059530Z",
     "shell.execute_reply": "2022-05-13T16:43:08.058996Z"
    }
   },
   "outputs": [],
   "source": [
    "scriptname = 'src/dataset.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326d6026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:08.062197Z",
     "iopub.status.busy": "2022-05-13T16:43:08.062051Z",
     "iopub.status.idle": "2022-05-13T16:43:08.068143Z",
     "shell.execute_reply": "2022-05-13T16:43:08.067670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from src.init import *  \n",
    "verbose = False\n",
    "\n",
    "with open(args.url_loader) as json_file:\n",
    "    Imagenet_urls_ILSVRC_2016 = json.load(json_file)\n",
    "\n",
    "def clean_list(list_dir, patterns=['.DS_Store']):\n",
    "    for pattern in patterns:\n",
    "        if pattern in list_dir: list_dir.remove('.DS_Store')\n",
    "    return list_dir\n",
    "\n",
    "import imageio\n",
    "def get_image(img_url, timeout=3., min_content=3, verbose=verbose):\n",
    "    try:\n",
    "        img_resp = imageio.imread(img_url)\n",
    "        if (len(img_resp.shape) < min_content): # TODO : raise error when min_content is not reached\n",
    "            if verbose : print(f\"Url {img_url} does not have enough content\")\n",
    "            return False\n",
    "        else:\n",
    "            if verbose : print(f\"Success with url {img_url}\")\n",
    "            return img_resp\n",
    "    except Exception as e:\n",
    "        if verbose : print(f\"Failed with {e} for url {img_url}\")\n",
    "        return False # did not work\n",
    "\n",
    "import hashlib # jah.\n",
    "\n",
    "# train, val and test folders\n",
    "    \n",
    "list_urls = {}\n",
    "list_img_name_used = {}\n",
    "for task in class_wnids :\n",
    "    list_urls[task] = {}\n",
    "    list_img_name_used[task] = {}\n",
    "    for goal in class_wnids[task]:\n",
    "        list_urls[task][goal] = {}\n",
    "        for class_wnid in class_wnids[task][goal]:\n",
    "            list_urls[task][goal][class_wnid] = Imagenet_urls_ILSVRC_2016[str(class_wnid)]\n",
    "        np.random.shuffle(list_urls[task][goal][class_wnid])\n",
    "        list_img_name_used[task][goal] = []\n",
    "            \n",
    "    \n",
    "# train, val and test folders\n",
    "for task in args.tasks:\n",
    "    pprint(f'Task \\\"{task}\\\"')\n",
    "    for folder in args.folders :\n",
    "        print(f'Folder \\\"{folder}\\\"')\n",
    "        filename = f'results/{datetag}_dataset_{folder}_{args.HOST}.json'\n",
    "        columns = ['task', 'goal', 'img_url', 'img_name', 'is_flickr', 'dt', 'worked', 'class_wnid', 'class_name']\n",
    "        if os.path.isfile(filename):\n",
    "            df_dataset = pd.read_json(filename)\n",
    "        else:\n",
    "            df_dataset = pd.DataFrame([], columns=columns)\n",
    "        for goal in args.goals :\n",
    "            print(f'Scraping images for task \\\"{task}\\\" : {goal} ')\n",
    "            task_goal_folder = os.path.join(paths[task][folder], goal)\n",
    "            os.makedirs(task_goal_folder, exist_ok=True)\n",
    "            list_img_name_used[task][goal] += clean_list(os.listdir(os.path.join(paths[task][folder], goal))) # join two lists\n",
    "            while (len(clean_list(os.listdir(task_goal_folder))) < N_images_per_class[folder]):\n",
    "                class_wnid = np.random.choice(class_wnids[task][goal], 1)[0]\n",
    "                pos = class_wnids[task][goal].index(class_wnid)\n",
    "                class_name = reverse_id_labels[class_wnid]\n",
    "                # pick and remove element from shuffled list\n",
    "                if len(list_urls[task][goal][class_wnid])==0:\n",
    "                    del class_wnids[task][goal][pos], list_urls[task][goal][class_wnid]\n",
    "                else:\n",
    "                    img_url = list_urls[task][goal][class_wnid].pop()\n",
    "\n",
    "                    if len(df_dataset[df_dataset['img_url']==img_url])==0 : # we have not yet tested this URL yet\n",
    "\n",
    "                        # Transform URL into filename\n",
    "                        # https://laurentperrinet.github.io/sciblog/posts/2018-06-13-generating-an-unique-seed-for-a-given-filename.html\n",
    "                        img_name = hashlib.sha224(img_url.encode('utf-8')).hexdigest() + '.png'\n",
    "\n",
    "                        if img_url.split('.')[-1] in ['.tiff', '.bmp', 'jpe', 'gif']:\n",
    "                            if verbose: print('Bad extension for the img_url', img_url)\n",
    "                            worked, dt = False, 0.\n",
    "                        # make sure it was not used in other folders\n",
    "                        elif not (img_name in list_img_name_used[task][goal]):\n",
    "                            tic = time.time()\n",
    "                            img_content = get_image(img_url, verbose=verbose)\n",
    "                            dt = time.time() - tic\n",
    "                            worked = img_content is not False\n",
    "                            if worked:\n",
    "                                if verbose : print('Good URl, now saving', img_url, ' in', task_goal_folder, ' as', img_name)\n",
    "                                imageio.imsave(os.path.join(task_goal_folder, img_name), img_content, format='png')\n",
    "                                list_img_name_used[task][goal].append(img_name)\n",
    "                        df_dataset.loc[len(df_dataset.index)] = {'task': task, 'goal': goal, 'img_url':img_url, 'img_name':img_name, 'is_flickr':1 if 'flickr' in img_url else 0, 'dt':dt,\n",
    "                                        'worked':worked, 'class_wnid':class_wnid, 'class_name':class_name}\n",
    "                        df_dataset.to_json(filename)\n",
    "                        print(f'\\r{len(clean_list(os.listdir(task_goal_folder)))} / {N_images_per_class[folder]}', end='\\n' if verbose else '', flush=not verbose)\n",
    "\n",
    "        if (len(clean_list(os.listdir(task_goal_folder))) < N_images_per_class[folder]) and (len(list_urls[task][goal]) == 0): \n",
    "            print('Not enough working url to complete the dataset') \n",
    "\n",
    "    df_dataset.to_json(filename)\n",
    "\n",
    "print('\\n')\n",
    "pprint(f'Some random images :')\n",
    "import imageio\n",
    "N_image_i = 4\n",
    "image_plot_paths = {}\n",
    "task_folder = {}\n",
    "x = 0\n",
    "folder = 'train'\n",
    "fig, axs = plt.subplots(len(args.tasks)*len(args.goals), N_image_i, figsize=(fig_width, fig_width))\n",
    "for task in args.tasks:\n",
    "    for goal in args.goals:\n",
    "        task_goal_folder = os.path.join(paths[task][folder], goal)\n",
    "        image_plot_paths = os.listdir(task_goal_folder)\n",
    "        for i_image in np.arange(N_image_i):\n",
    "            ax = axs[x][i_image]\n",
    "            path = os.path.join(task_goal_folder, image_plot_paths[i_image])\n",
    "            ax.imshow(imageio.imread(path))\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])  \n",
    "            if i_image%5 == 0:\n",
    "                ax.set_ylabel((task+' '+goal), fontsize = 18)\n",
    "        x +=1\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af8e3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:08.070769Z",
     "iopub.status.busy": "2022-05-13T16:43:08.070538Z",
     "iopub.status.idle": "2022-05-13T16:43:08.644151Z",
     "shell.execute_reply": "2022-05-13T16:43:08.643679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-13_config_args.json\n",
      "On date 2022-05-13 , Running benchmark on host babbage  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/home/laurent/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/home/laurent/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/metagit/JNJER/2022-03_UltraFastCat/src/dataset.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      3\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(args\u001b[38;5;241m.\u001b[39murl_loader) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n",
      "File \u001b[0;32m~/metagit/JNJER/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovenances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momw_prov\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m provdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1284\u001b[0m provdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1285\u001b[0m fileids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_omw_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m()\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m   1287\u001b[0m     prov, langfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(fileid)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/home/laurent/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90dd68",
   "metadata": {},
   "source": [
    "## Transfer learning and dataset config\n",
    "\n",
    "On the model.py we first define the `transforme`functions for the datasets. To perform image augmentation we apply the Pytorch `AutoAugment` function to the `train` and `val` dataset. I also add a grayscale, shuffle and resize function in order to test different training strategies and test the networks on various conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed8f950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:08.649209Z",
     "iopub.status.busy": "2022-05-13T16:43:08.649046Z",
     "iopub.status.idle": "2022-05-13T16:43:08.653039Z",
     "shell.execute_reply": "2022-05-13T16:43:08.652146Z"
    }
   },
   "outputs": [],
   "source": [
    "scriptname = 'src/model.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b4b0c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:08.656336Z",
     "iopub.status.busy": "2022-05-13T16:43:08.655979Z",
     "iopub.status.idle": "2022-05-13T16:43:08.665184Z",
     "shell.execute_reply": "2022-05-13T16:43:08.664442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from src.init import *  \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "\n",
    "class ShufflePatches(object): #https://stackoverflow.com/questions/66962837/shuffle-patches-in-image-batch\n",
    "    def __init__(self, patch_size):\n",
    "        self.ps = patch_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # divide the batch of images into non-overlapping patches\n",
    "        u = nnf.unfold(x.unsqueeze(dim=0), kernel_size=self.ps, stride=self.ps, padding=0)\n",
    "        # permute the patches of each image in the batch\n",
    "        pu = torch.cat([b_[:, torch.randperm(b_.shape[-1])][None,...] for b_ in u], dim=0)\n",
    "        # fold the permuted patches back together\n",
    "        f = nnf.fold(pu, x.shape[-2:], kernel_size=self.ps, stride=self.ps, padding=0)\n",
    "        return f.squeeze(dim=0)\n",
    "\n",
    "# normalization used to train VGG\n",
    "# see https://pytorch.org/hub/pytorch_vision_vgg/\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "transforms_norm = transforms.Normalize(mean=mean, std=std) # to normalize colors on the imagenet dataset\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "from scipy import stats\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "image_datasets = {}\n",
    "dataloaders = {}\n",
    "dataset_sizes = {}\n",
    "\n",
    "# VGG-16 datasets initialisation\n",
    "def datasets_transforms(image_size=args.image_size, p=0, shuffle=args.image_size , num_workers=1, angle=0, batch_size=args.batch_size, **kwargs):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((int(image_size), int(image_size))),\n",
    "            transforms.AutoAugment(), # https://pytorch.org/vision/master/transforms.html#torchvision.transforms.AutoAugment\n",
    "            transforms.RandomGrayscale(p=p),\n",
    "            transforms.ToTensor(),      # Convert the image to pyTorch Tensor data type.\n",
    "            transforms_norm ]),\n",
    "\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((int(image_size), int(image_size))),\n",
    "            transforms.AutoAugment(), \n",
    "            transforms.RandomGrayscale(p=p),\n",
    "            transforms.ToTensor(),    \n",
    "            transforms_norm ]),\n",
    "\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((int(image_size), int(image_size))),\n",
    "            transforms.RandomGrayscale(p=p),\n",
    "            transforms.ToTensor(),\n",
    "            transforms_norm, \n",
    "            ShufflePatches(patch_size=(shuffle,shuffle)) ]), # our new transform to shuffle patches of images\n",
    "    }\n",
    "    \n",
    "    for task in args.tasks:\n",
    "        image_datasets[task] = {\n",
    "            folder: datasets.ImageFolder(\n",
    "                paths[task][folder], \n",
    "                transform=data_transforms[folder]\n",
    "            )\n",
    "            for folder in args.folders\n",
    "        }\n",
    "\n",
    "        dataloaders[task] = {\n",
    "            folder: torch.utils.data.DataLoader(\n",
    "                image_datasets[task][folder], batch_size=batch_size,\n",
    "                shuffle=True if folder == \"train\" else False, num_workers=num_workers\n",
    "            )\n",
    "            for folder in args.folders\n",
    "        }\n",
    "\n",
    "        dataset_sizes[task] = {folder: len(image_datasets[task][folder]) for folder in args.folders}\n",
    "\n",
    "    return dataset_sizes, dataloaders, image_datasets, data_transforms\n",
    "\n",
    "(dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size)\n",
    "\n",
    "for task in args.tasks :\n",
    "    pprint(task)\n",
    "    for folder in args.folders : print(f\"Loaded {dataset_sizes[task][folder]} images under {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc11549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T16:43:08.668727Z",
     "iopub.status.busy": "2022-05-13T16:43:08.668250Z",
     "iopub.status.idle": "2022-05-13T16:43:09.206871Z",
     "shell.execute_reply": "2022-05-13T16:43:09.206269Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2022-05-13_config_args.json\n",
      "On date 2022-05-13 , Running benchmark on host babbage  with device cuda\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/home/laurent/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/home/laurent/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/metagit/JNJER/2022-03_UltraFastCat/src/model.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnnf\u001b[39;00m\n",
      "File \u001b[0;32m~/metagit/JNJER/2022-03_UltraFastCat/src/init.py:116\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m reverse_id_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_img, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Imagenet_urls_ILSVRC_2016):\n\u001b[0;32m--> 116\u001b[0m     syn_\u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynset_from_pos_and_offset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(img_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    117\u001b[0m     reverse_id_labels[img_id] \u001b[38;5;241m=\u001b[39m syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname()\n\u001b[1;32m    118\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(syn_\u001b[38;5;241m.\u001b[39mlemmas()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovenances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momw_prov\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m provdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1284\u001b[0m provdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1285\u001b[0m fileids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_omw_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m()\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m   1287\u001b[0m     prov, langfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(fileid)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/home/laurent/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
